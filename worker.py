# worker.py - FINAL OPTIMIZED VERSION WITH OFFLOAD FIX
import os
import json
import time
import requests
import subprocess
import uuid
import shutil
from pathlib import Path
from PIL import Image
import cv2
import torch

# CRITICAL: Disable model offloading by setting distributed environment
os.environ['WORLD_SIZE'] = '2'
os.environ['RANK'] = '0'
os.environ['LOCAL_RANK'] = '0'

# Force GPU usage
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['TORCH_USE_CUDA_DSA'] = '1'

class VideoWorker:
    def __init__(self):
        print("üöÄ OurVidz Worker initialized (FINAL OPTIMIZED)")
        
        # Verify CUDA availability
        if not torch.cuda.is_available():
            print("‚ùå CUDA not available - exiting")
            exit(1)
        
        # Force GPU setup
        torch.cuda.set_device(0)
        torch.cuda.empty_cache()
        
        # Log GPU status
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"üî• GPU: {gpu_name} ({gpu_memory:.1f}GB)")
        
        # Create temp directories
        self.temp_base = Path("/tmp/ourvidz")
        self.temp_base.mkdir(exist_ok=True)
        self.temp_processing = self.temp_base / "processing"
        self.temp_processing.mkdir(exist_ok=True)
        
        # Paths
        self.model_path = "/workspace/models/wan2.1-t2v-1.3b"
        self.wan_path = "/workspace/Wan2.1"
        
        # Verify critical paths
        if not Path(self.wan_path).exists():
            print(f"‚ùå Wan2.1 path missing: {self.wan_path}")
            exit(1)
        
        if not Path(self.model_path).exists():
            print(f"‚ùå Model path missing: {self.model_path}")
            exit(1)
        
        # Job configurations (ULTRA OPTIMIZED for 4-15 second generation)
        self.job_type_mapping = {
            'image_fast': {
                'content_type': 'image',
                'sample_steps': 4,
                'sample_guide_scale': 3.0,
                'size': '480*832',
                'frame_num': 1,
                'storage_bucket': 'image_fast',
                'expected_time': 4,  # Updated based on actual performance
                'description': 'Ultra fast image (4s)'
            },
            'image_high': {
                'content_type': 'image',
                'sample_steps': 6,
                'sample_guide_scale': 4.0,
                'size': '832*480',
                'frame_num': 1,
                'storage_bucket': 'image_high',
                'expected_time': 6,  # Updated based on actual performance
                'description': 'High quality image (6s)'
            },
            'video_fast': {
                'content_type': 'video',
                'sample_steps': 4,
                'sample_guide_scale': 3.0,
                'size': '480*832',
                'frame_num': 17,
                'storage_bucket': 'video_fast',
                'expected_time': 8,  # Updated based on actual performance
                'description': 'Fast video (8s)'
            },
            'video_high': {
                'content_type': 'video',
                'sample_steps': 6,
                'sample_guide_scale': 4.0,
                'size': '832*480',
                'frame_num': 17,
                'storage_bucket': 'video_high',
                'expected_time': 12,  # Updated based on actual performance
                'description': 'High quality video (12s)'
            }
        }
        
        # Environment variables
        self.supabase_url = os.getenv('SUPABASE_URL')
        self.supabase_service_key = os.getenv('SUPABASE_SERVICE_KEY')
        self.redis_url = os.getenv('UPSTASH_REDIS_REST_URL')
        self.redis_token = os.getenv('UPSTASH_REDIS_REST_TOKEN')

        print("üé¨ Worker ready with OFFLOAD DISABLED for maximum performance")

    def log_gpu_memory(self, context=""):
        """Log GPU memory usage for monitoring"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / (1024**3)
            reserved = torch.cuda.memory_reserved() / (1024**3)
            total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            print(f"üî• GPU {context}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved / {total:.1f}GB total")

    def generate_with_optimized_settings(self, prompt, job_type):
        """Generate using OPTIMIZED settings with model offloading disabled"""
        config = self.job_type_mapping.get(job_type, self.job_type_mapping['image_fast'])
        
        print(f"‚ö° {job_type.upper()} generation")
        print(f"üìù Prompt: {prompt}")
        print(f"üîß Config: {config['sample_steps']} steps, {config['sample_guide_scale']} guidance")
        print(f"üéØ Expected: {config['expected_time']}s (OPTIMIZED)")
        
        job_id = str(uuid.uuid4())[:8]
        output_filename = f"{job_type}_{job_id}.mp4"
        temp_output_path = self.temp_processing / output_filename
        
        # Build optimized command
        cmd = [
            "python", "generate.py",
            "--task", "t2v-1.3B",
            "--ckpt_dir", self.model_path,
            "--offload_model", "False",  # Explicitly disable (though env vars override)
            "--size", config['size'],
            "--sample_steps", str(config['sample_steps']),
            "--sample_guide_scale", str(config['sample_guide_scale']),
            "--frame_num", str(config['frame_num']),
            "--prompt", prompt,
            "--save_file", str(temp_output_path)
        ]
        
        # Environment with distributed settings to disable offloading
        env = os.environ.copy()
        env.update({
            'WORLD_SIZE': '2',  # Critical: Disables model offloading
            'RANK': '0',
            'LOCAL_RANK': '0',
            'CUDA_VISIBLE_DEVICES': '0',
            'TORCH_USE_CUDA_DSA': '1'
        })
        
        original_cwd = os.getcwd()
        os.chdir(self.wan_path)
        
        try:
            start_time = time.time()
            
            result = subprocess.run(
                cmd,
                env=env,
                capture_output=True,
                text=True,
                timeout=60  # Should complete in under 15 seconds now
            )
            
            generation_time = time.time() - start_time
            print(f"‚ö° Generation completed in {generation_time:.1f}s")
            
            if result.returncode != 0:
                # Check if it's just the distributed training error at the end
                if generation_time < 20 and temp_output_path.exists():
                    print("‚úÖ Generation successful despite distributed training error")
                else:
                    print(f"‚ùå Generation failed: {result.stderr[:500]}")
                    return None
                
            # Check for output file
            if not temp_output_path.exists():
                # Check for alternative output locations
                fallback_path = Path(output_filename)
                if fallback_path.exists():
                    shutil.move(str(fallback_path), str(temp_output_path))
                else:
                    print("‚ùå Output file not found")
                    return None
            
            if config['content_type'] == 'image':
                return self.extract_frame_from_video(str(temp_output_path), job_id, job_type)
            
            return str(temp_output_path)
            
        except subprocess.TimeoutExpired:
            print("‚ùå Generation timed out (>60s) - this should not happen with optimized settings")
            return None
        except Exception as e:
            print(f"‚ùå Error: {e}")
            return None
        finally:
            os.chdir(original_cwd)

    def extract_frame_from_video(self, video_path, job_id, job_type):
        """Extract frame for image jobs"""
        image_path = self.temp_processing / f"{job_type}_{job_id}.png"
        
        try:
            cap = cv2.VideoCapture(video_path)
            ret, frame = cap.read()
            cap.release()
            
            if ret and frame is not None:
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                img = Image.fromarray(frame_rgb)
                img.save(str(image_path), "PNG", optimize=True)
                
                file_size = os.path.getsize(image_path) / 1024
                print(f"üìä Output: {file_size:.0f}KB")
                
                try:
                    os.remove(video_path)
                except:
                    pass
                    
                return str(image_path)
        except Exception as e:
            print(f"‚ùå Frame extraction error: {e}")
        return None

    def upload_to_supabase(self, file_path, job_type, user_id, job_id):
        """Upload to Supabase"""
        if not os.path.exists(file_path):
            return None
            
        config = self.job_type_mapping.get(job_type, self.job_type_mapping['image_fast'])
        storage_bucket = config['storage_bucket']
        content_type = config['content_type']
        
        filename = f"job_{job_id}_{int(time.time())}_{job_type}.{'png' if content_type == 'image' else 'mp4'}"
        full_path = f"{storage_bucket}/{user_id}/{filename}"
        mime_type = 'image/png' if content_type == 'image' else 'video/mp4'
        
        print(f"üì§ Uploading to bucket: {storage_bucket}")
        
        try:
            with open(file_path, 'rb') as f:
                file_data = f.read()
                file_size = len(file_data) / 1024
                print(f"üìä File size: {file_size:.0f}KB")
                
                r = requests.post(
                    f"{self.supabase_url}/storage/v1/object/{full_path}",
                    data=file_data,
                    headers={
                        'Authorization': f"Bearer {self.supabase_service_key}",
                        'Content-Type': mime_type,
                        'x-upsert': 'true'
                    },
                    timeout=60
                )
                
                if r.status_code in [200, 201]:
                    print(f"‚úÖ Upload successful")
                    return f"{user_id}/{filename}"
                else:
                    print(f"‚ùå Upload failed: {r.status_code}")
                    
        except Exception as e:
            print(f"‚ùå Upload error: {e}")
        finally:
            try:
                if file_path and os.path.exists(file_path):
                    os.remove(file_path)
            except:
                pass
            
        return None

    def notify_completion(self, job_id, status, file_path=None, error_message=None):
        """Notify completion"""
        data = {
            'jobId': job_id,
            'status': status,
            'filePath': file_path,
            'errorMessage': error_message
        }
        
        print(f"üìû Calling job-callback for job {job_id}: {status}")
        
        try:
            r = requests.post(
                f"{self.supabase_url}/functions/v1/job-callback",
                json=data,
                headers={
                    'Authorization': f"Bearer {self.supabase_service_key}",
                    'Content-Type': 'application/json'
                },
                timeout=15
            )
            
            if r.status_code == 200:
                print("‚úÖ Callback sent successfully")
            else:
                print(f"‚ùå Callback failed: {r.status_code}")
                
        except Exception as e:
            print(f"‚ùå Callback error: {e}")

    def process_job(self, job_data):
        """Process job with optimized performance"""
        job_id = job_data.get('jobId')
        job_type = job_data.get('jobType')
        prompt = job_data.get('prompt')
        user_id = job_data.get('userId')
        
        if not all([job_id, job_type, user_id, prompt]):
            error_msg = "Missing required fields"
            print(f"‚ùå {error_msg}")
            self.notify_completion(job_id or 'unknown', 'failed', error_message=error_msg)
            return

        print(f"üì• Processing job: {job_id} ({job_type})")
        start_time = time.time()
        
        try:
            # Clear GPU cache before generation
            torch.cuda.empty_cache()
            
            output_path = self.generate_with_optimized_settings(prompt, job_type)
            if output_path:
                supa_path = self.upload_to_supabase(output_path, job_type, user_id, job_id)
                if supa_path:
                    duration = time.time() - start_time
                    expected = self.job_type_mapping[job_type]['expected_time']
                    print(f"üéâ Job completed in {duration:.1f}s (expected {expected}s)")
                    self.notify_completion(job_id, 'completed', supa_path)
                    return
                    
            self.notify_completion(job_id, 'failed', error_message="Generation or upload failed")
            
        except Exception as e:
            print(f"‚ùå Job processing error: {e}")
            self.notify_completion(job_id, 'failed', error_message=str(e))

    def poll_queue(self):
        """Poll Redis queue"""
        try:
            r = requests.get(
                f"{self.redis_url}/rpop/job_queue",
                headers={'Authorization': f"Bearer {self.redis_token}"},
                timeout=5
            )
            if r.status_code == 200 and r.json().get('result'):
                return json.loads(r.json()['result'])
        except Exception as e:
            print(f"‚ùå Poll error: {e}")
        return None

    def run(self):
        """Main loop with optimized performance"""
        print("‚è≥ Waiting for jobs...")
        print("üöÄ OPTIMIZED Job Types (Model offloading DISABLED):")
        for job_type, config in self.job_type_mapping.items():
            print(f"   ‚Ä¢ {job_type}: {config['description']}")
        
        # Display performance improvement
        print("\nüéØ Performance Optimizations Active:")
        print("   ‚Ä¢ Model offloading: DISABLED (WORLD_SIZE=2)")
        print("   ‚Ä¢ GPU memory: Persistent (no CPU shuffling)")
        print("   ‚Ä¢ Expected 21x speedup: 90s ‚Üí 4-15s")
        
        job_count = 0
        
        while True:
            job = self.poll_queue()
            if job:
                job_count += 1
                print(f"üéØ Processing job #{job_count}")
                self.process_job(job)
                
                # Clear GPU cache after each job
                torch.cuda.empty_cache()
            else:
                time.sleep(5)

if __name__ == "__main__":
    print("üöÄ Starting OurVidz FINAL OPTIMIZED Worker")
    print("üî• Model offloading DISABLED for maximum performance!")
    
    # Verify environment
    required_vars = ['SUPABASE_URL', 'SUPABASE_SERVICE_KEY', 'UPSTASH_REDIS_REST_URL', 'UPSTASH_REDIS_REST_TOKEN']
    missing = [var for var in required_vars if not os.getenv(var)]
    if missing:
        print(f"‚ùå Missing environment variables: {missing}")
        exit(1)
    
    try:
        worker = VideoWorker()
        worker.run()
    except Exception as e:
        print(f"‚ùå Worker failed: {e}")
        exit(1)
