# wan_worker.py - CLEANUP VERSION (WAN 1.3B + QWEN BASE ONLY)
# FIXES: Correct task names for 1.3B model, proper I2V support for reference frames, auto URL registration
# MAJOR FIX: Use correct 1.3B tasks (t2v-1.3B, i2v not flf2v)
# PARAMETER FIX: Consistent parameter names (job_id, assets) with edge function
# REFERENCE STRENGTH FIX: Adjust sample_guide_scale based on reference strength
# AUTO-REGISTRATION FIX: Detect RunPod URL and register with Supabase automatically
# THREAD-SAFE FIX: Replace signal-based timeouts with thread-safe concurrent.futures
# CLEANUP: Removed Qwen Instruct model code - WAN worker focuses on video generation + Base model enhancement only
# Date: July 30, 2025

import os
import json
import time
import torch
import requests
import subprocess
import tempfile
import mimetypes
import fcntl
import glob
import io
import threading
from pathlib import Path
from PIL import Image
from transformers import AutoTokenizer, AutoModelForCausalLM
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
import concurrent.futures

# Flask imports for frontend enhancement API
try:
    from flask import Flask, request, jsonify
    FLASK_AVAILABLE = True
    # Create Flask app at module level
    app = Flask(__name__)
except ImportError:
    print("‚ö†Ô∏è Flask not available - frontend enhancement API will be disabled")
    FLASK_AVAILABLE = False
    app = None

# Auto-registration imports
import socket
from datetime import datetime

class EnhancementSystemPrompts:
    """Intelligent system prompts for model-specific enhancement"""
    
    @staticmethod
    def get_sdxl_system_prompt(job_type="sdxl_image_fast", quality="fast"):
        """SDXL LUSTIFY-specific system prompt"""
        base_prompt = """You are an expert AI prompt engineer specializing in SDXL LUSTIFY model optimization and adult content generation.

CRITICAL REQUIREMENTS:
- Target Model: SDXL LUSTIFY v2.0 (75-token optimal, 150-token maximum)
- Content Focus: Adult/NSFW content with anatomical accuracy
- Quality Priority: Photorealistic details, professional lighting, technical excellence

ENHANCEMENT STRATEGY:
1. ANATOMY FIRST: Ensure accurate proportions, realistic features, natural poses
2. QUALITY TAGS: Add professional photography terms (4K, sharp focus, detailed)
3. LIGHTING: Specify cinematic lighting (soft lighting, natural light, studio lighting)
4. TECHNICAL: Include quality modifiers (masterpiece, best quality, ultra-detailed)
5. COMPRESS: Optimize for 75-token sweet spot while preserving key details

SDXL-SPECIFIC OPTIMIZATION:
- Use proven SDXL quality tags: "masterpiece, best quality, ultra detailed"
- Emphasize lighting: "professional photography, soft lighting, detailed"
- Anatomical accuracy: "realistic proportions, natural pose, detailed features"
- Avoid unnecessary words, prioritize visual impact terms
- Balance detail with token efficiency"""

        if quality == "high":
            base_prompt += """

HIGH QUALITY MODE:
- Extend to 100-120 tokens for maximum detail
- Add advanced technical terms: "photorealistic, hyperdetailed, professional grade"
- Include specific camera settings: "85mm lens, shallow depth of field"
- Enhanced lighting details: "rim lighting, volumetric lighting, perfect exposure"
"""
        
        return base_prompt

    @staticmethod 
    def get_wan_system_prompt(job_type="video_fast", quality="fast"):
        """WAN 2.1-specific system prompt"""
        base_prompt = """You are an expert AI prompt engineer specializing in WAN 2.1 video generation and temporal consistency.

CRITICAL REQUIREMENTS:
- Target Model: WAN 2.1 T2V 1.3B (motion-focused, 5-second videos)
- Content Focus: Temporal consistency, smooth motion, cinematic quality
- Quality Priority: Motion realism, scene coherence, professional cinematography

ENHANCEMENT STRATEGY:
1. MOTION FIRST: Describe natural, fluid movements and transitions
2. TEMPORAL CONSISTENCY: Ensure elements maintain coherence across frames
3. CINEMATOGRAPHY: Add professional camera work (smooth pans, steady shots)
4. SCENE SETTING: Establish clear environment and spatial relationships  
5. TECHNICAL QUALITY: Video-specific quality terms (smooth motion, stable)

WAN-SPECIFIC OPTIMIZATION:
- Motion descriptions: "smooth movement, natural motion, fluid transitions"
- Temporal stability: "consistent lighting, stable composition, coherent scene"
- Cinematography: "professional camera work, smooth pans, steady shots"
- Video quality: "high framerate, smooth motion, temporal consistency"
- Scene coherence: "well-lit environment, clear spatial relationships"

TOKEN STRATEGY: 150-250 tokens optimal for detailed motion description"""

        if "7b_enhanced" in job_type:
            base_prompt += """

QWEN 7B ENHANCED MODE:
- Leverage full 7B model capabilities for superior enhancement
- Advanced cinematography: "dynamic camera angles, professional composition"
- Complex motion: "multi-layered motion, realistic physics, natural timing"
- Enhanced storytelling: "narrative coherence, emotional resonance"
- Technical excellence: "broadcast quality, professional grade, cinema-level"
"""

        return base_prompt

    @staticmethod
    def get_enhancement_context(job_type, quality_level, model_target):
        """Generate contextual information for AI enhancement"""
        return {
            "job_type": job_type,
            "quality_level": quality_level, 
            "target_model": model_target,
            "token_target": 75 if "sdxl" in job_type else 200,
            "content_type": "video" if "video" in job_type else "image",
            "enhancement_level": "enhanced" if "7b" in job_type else "standard"
        }

def create_enhanced_messages(original_prompt, job_type="sdxl_image_fast", quality="fast"):
    """Create contextually-aware messages for AI enhancement"""
    
    # Determine system prompt based on job type
    if "sdxl" in job_type:
        system_prompt = EnhancementSystemPrompts.get_sdxl_system_prompt(job_type, quality)
        model_context = "SDXL LUSTIFY"
    elif "video" in job_type or "image" in job_type:
        system_prompt = EnhancementSystemPrompts.get_wan_system_prompt(job_type, quality)
        model_context = "WAN 2.1"
    else:
        # Fallback
        system_prompt = EnhancementSystemPrompts.get_sdxl_system_prompt(job_type, quality)
        model_context = "SDXL LUSTIFY"
    
    # Create enhancement context
    context = EnhancementSystemPrompts.get_enhancement_context(job_type, quality, model_context)
    
    # Build intelligent user prompt with context
    user_prompt = f"""ENHANCEMENT REQUEST:
Model Target: {context['target_model']}
Content Type: {context['content_type'].title()}
Quality Level: {context['quality_level'].title()}
Token Target: {context['token_target']} tokens optimal
Enhancement Level: {context['enhancement_level'].title()}

Original Prompt: "{original_prompt}"

Task: Enhance this prompt according to the system requirements above. Focus on {model_context}-specific optimization while maintaining the original creative intent. Ensure the enhancement is optimized for {context['content_type']} generation with {context['quality_level']} quality settings."""

    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ]

class TimeoutException(Exception):
    """Custom exception for timeouts - now thread-safe"""
    pass

def run_with_timeout(func, timeout_seconds, *args, **kwargs):
    """Thread-safe timeout wrapper using concurrent.futures"""
    with ThreadPoolExecutor(max_workers=1) as executor:
        future = executor.submit(func, *args, **kwargs)
        try:
            return future.result(timeout=timeout_seconds)
        except concurrent.futures.TimeoutError:
            raise TimeoutException(f"Operation timed out after {timeout_seconds} seconds")

def detect_runpod_url():
    """Detect current RunPod URL using official environment variables"""
    try:
        # Method 1: Official RUNPOD_POD_ID (most reliable)
        pod_id = os.environ.get('RUNPOD_POD_ID')
        if pod_id:
            url = f"https://{pod_id}-7860.proxy.runpod.net"
            print(f"‚úÖ URL from RUNPOD_POD_ID: {url}")
            return url
        
        # Method 2: Fallback to hostname parsing
        hostname = socket.gethostname()
        print(f"üîç Hostname fallback: {hostname}")
        
        if 'runpod' in hostname.lower() or len(hostname) > 8:
            # Extract first part as pod ID
            pod_id = hostname.split('-')[0] if '-' in hostname else hostname
            url = f"https://{pod_id}-7860.proxy.runpod.net"
            print(f"‚úÖ URL from hostname: {url}")
            return url
        
        print("‚ùå Could not detect RunPod URL")
        return None
        
    except Exception as e:
        print(f"‚ùå URL detection error: {e}")
        return None

def validate_worker_url(url):
    """Test if the detected URL actually works"""
    try:
        print(f"üîç Validating worker URL: {url}")
        
        # Test health endpoint with timeout
        response = requests.get(f"{url}/health", timeout=10)
        
        if response.status_code == 200:
            print(f"‚úÖ Worker URL validated: {url}")
            return True
        else:
            print(f"‚ùå Worker not responding: {response.status_code}")
            return False
            
    except requests.exceptions.RequestException as e:
        print(f"‚ùå URL validation failed: {e}")
        return False

def auto_register_worker_url():
    """Auto-register worker URL with Supabase after Flask is ready"""
    try:
        print("üåê Starting worker URL auto-registration...")
        
        # Detect current URL using RunPod environment variables
        detected_url = detect_runpod_url()
        if not detected_url:
            print("‚ö†Ô∏è Could not detect RunPod URL - skipping auto-registration")
            return False
        
        # ‚úÖ IMPROVED: Skip URL validation during initial registration
        # Flask might not be fully ready yet, so we'll register the URL anyway
        # The periodic health monitoring will validate it later
        print(f"üåê Detected URL: {detected_url}")
        
        # Debug: Check environment variables
        print(f"üîç SUPABASE_URL: {os.environ.get('SUPABASE_URL', 'NOT SET')}")
        print(f"üîç SUPABASE_SERVICE_KEY: {'SET' if os.environ.get('SUPABASE_SERVICE_KEY') else 'NOT SET'}")
        
        # Register with Supabase via update-worker-url edge function
        registration_data = {
            "worker_url": detected_url,
            "auto_registered": True,
            "registration_method": "wan_worker_self_registration",
            "detection_method": "RUNPOD_POD_ID",
            "timestamp": datetime.now().isoformat()
        }
        
        print(f"üìù Registering with Supabase: {detected_url}")
        print(f"üìÑ Registration data: {registration_data}")
        
        edge_function_url = f"{os.environ['SUPABASE_URL']}/functions/v1/update-worker-url"
        print(f"üåê Edge function URL: {edge_function_url}")
        
        response = requests.post(
            edge_function_url,
            headers={
                "Authorization": f"Bearer {os.environ['SUPABASE_SERVICE_KEY']}",
                "Content-Type": "application/json"
            },
            json=registration_data,
            timeout=15
        )
        
        print(f"üìÑ Response status: {response.status_code}")
        print(f"üìÑ Response headers: {dict(response.headers)}")
        
        try:
            result = response.json()
            print(f"üìÑ Response JSON: {result}")
        except:
            print(f"üìÑ Response text: {response.text}")
            result = {}
        
        if response.status_code == 200:
            if result.get('success'):
                print(f"‚úÖ Worker URL auto-registered successfully!")
                print(f"üéØ Registration confirmed: {detected_url}")
                return True
            else:
                print(f"‚ùå Registration failed: {result.get('message', 'Unknown error')}")
                # Try fallback registration method
                return fallback_registration(detected_url)
        else:
            print(f"‚ùå Registration HTTP error: {response.status_code}")
            print(f"üìÑ Response: {response.text}")
            # Try fallback registration method
            return fallback_registration(detected_url)
            
    except Exception as e:
        print(f"‚ùå Auto-registration error: {e}")
        return False

def fallback_registration(detected_url):
    """Fallback registration method using direct Supabase REST API"""
    try:
        print("üîÑ Attempting fallback registration via REST API...")
        
        # Try to update system_config table directly
        response = requests.patch(
            f"{os.environ['SUPABASE_URL']}/rest/v1/system_config",
            json={
                "key": "worker_url",
                "value": detected_url,
                "updated_at": datetime.now().isoformat()
            },
            headers={
                "Authorization": f"Bearer {os.environ['SUPABASE_SERVICE_KEY']}",
                "Content-Type": "application/json",
                "Prefer": "return=minimal"
            },
            timeout=10
        )
        
        if response.status_code in [200, 201, 204]:
            print(f"‚úÖ Fallback registration successful: {detected_url}")
            return True
        else:
            print(f"‚ùå Fallback registration failed: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"‚ùå Fallback registration error: {e}")
        return False

def start_periodic_health_monitoring():
    """Start periodic health monitoring and URL validation"""
    def health_monitor():
        while True:
            try:
                time.sleep(300)  # Check every 5 minutes
                
                detected_url = detect_runpod_url()
                if detected_url and validate_worker_url(detected_url):
                    # Send health ping to keep registration active
                    requests.patch(
                        f"{os.environ['SUPABASE_URL']}/rest/v1/system_config",
                        json={
                            "config": {
                                "last_health_check": datetime.now().isoformat(),
                                "worker_status": "active"
                            }
                        },
                        headers={
                            "Authorization": f"Bearer {os.environ['SUPABASE_SERVICE_KEY']}",
                            "Content-Type": "application/json"
                        },
                        timeout=10
                    )
                    print("üíö Health monitoring ping sent")
                else:
                    print("‚ö†Ô∏è Health check failed - attempting re-registration")
                    auto_register_worker_url()
                    
            except Exception as e:
                print(f"‚ùå Health monitoring error: {e}")
    
    # Start health monitoring in background thread
    monitor_thread = threading.Thread(target=health_monitor, daemon=True)
    monitor_thread.start()
    print("‚úÖ Periodic health monitoring started")

class EnhancedWanWorker:
    def __init__(self):
        """Initialize Enhanced WAN Worker for 1.3B Model with Reference Support"""
        self.model_path = "/workspace/models/wan2.1-t2v-1.3b"
        self.wan_code_path = "/workspace/Wan2.1"
        
        # CRITICAL: Set environment variables immediately (VERIFIED FIX)
        os.environ['PYTHONPATH'] = '/workspace/python_deps/lib/python3.11/site-packages'
        os.environ['HF_HOME'] = '/workspace/models/huggingface_cache'
        os.environ['HUGGINGFACE_HUB_CACHE'] = '/workspace/models/huggingface_cache/hub'
        
        # Set memory fraction limit for WAN worker (30GB out of 48GB)
        if torch.cuda.is_available():
            torch.cuda.set_per_process_memory_fraction(0.63)  # 30GB / 48GB
            print("üß† Memory fraction set to 0.63 (30GB) for WAN worker")
        
        # UPDATED: Qwen 2.5-7B Base model path (no content filtering)
        self.hf_cache_path = "/workspace/models/huggingface_cache"
        self.qwen_model_path = "/workspace/models/huggingface_cache/hub/models--Qwen--Qwen2.5-7B/snapshots/d149729398750b98c0af14eb82c78cfe92750796"
        
        # Environment configuration
        self.supabase_url = os.getenv('SUPABASE_URL')
        self.supabase_service_key = os.getenv('SUPABASE_SERVICE_KEY')
        self.redis_url = os.getenv('UPSTASH_REDIS_REST_URL')
        self.redis_token = os.getenv('UPSTASH_REDIS_REST_TOKEN')
        
        # Model instances (loaded on demand)
        self.qwen_model = None
        self.qwen_tokenizer = None
        
        # Enhancement settings
        self.enhancement_timeout = 60
        self.max_enhancement_attempts = 2
        
        # FIXED: 1.3B Model Configurations with correct task names
        self.job_configs = {
            # Standard job types (no enhancement) - FIXED for 1.3B model
            'image_fast': {
                'task': 't2v-1.3B',            # ‚úÖ FIXED: Use t2v-1.3B for single frame (image)
                'size': '480*832',
                'sample_steps': 25,
                'sample_guide_scale': 6.5,
                'sample_solver': 'dpm++',     # ‚úÖ FIXED: Changed from unipc to dpm++ for WAN compatibility
                'sample_shift': 5.0,
                'frame_num': 1,                # Single frame for images
                'enhance_prompt': False,
                'expected_time': 25,
                'content_type': 'image',
                'file_extension': 'png'
            },
            'image_high': {
                'task': 't2v-1.3B',            # ‚úÖ FIXED: Use t2v-1.3B for single frame
                'size': '480*832',
                'sample_steps': 50,
                'sample_guide_scale': 7.5,
                'sample_solver': 'dpm++',     # ‚úÖ FIXED: Changed from unipc to dpm++ for WAN compatibility
                'sample_shift': 5.0,
                'frame_num': 1,
                'enhance_prompt': False,
                'expected_time': 40,
                'content_type': 'image',
                'file_extension': 'png'
            },
            'video_fast': {
                'task': 't2v-1.3B',            # ‚úÖ FIXED: Use t2v-1.3B for video
                'size': '480*832',
                'sample_steps': 25,
                'sample_guide_scale': 6.5,
                'sample_solver': 'dpm++',     # ‚úÖ FIXED: Changed from unipc to dpm++ for WAN compatibility
                'sample_shift': 5.0,
                'frame_num': 83,               # 83 frames for 5-second videos
                'enhance_prompt': False,
                'expected_time': 135,
                'content_type': 'video',
                'file_extension': 'mp4'
            },
            'video_high': {
                'task': 't2v-1.3B',            # ‚úÖ FIXED: Use t2v-1.3B for video
                'size': '480*832',
                'sample_steps': 50,
                'sample_guide_scale': 7.5,
                'sample_solver': 'dpm++',     # ‚úÖ FIXED: Changed from unipc to dpm++ for WAN compatibility
                'sample_shift': 5.0,
                'frame_num': 83,
                'enhance_prompt': False,
                'expected_time': 180,
                'content_type': 'video',
                'file_extension': 'mp4'
            },
            
            # Enhanced job types (with Qwen 7B Base enhancement) - FIXED for 1.3B
            'image7b_fast_enhanced': {
                'task': 't2v-1.3B',            # ‚úÖ FIXED: Use t2v-1.3B
                'size': '480*832',
                'sample_steps': 25,
                'sample_guide_scale': 6.5,
                'sample_solver': 'dpm++',     # ‚úÖ FIXED: Changed from unipc to dpm++ for WAN compatibility
                'sample_shift': 5.0,
                'frame_num': 1,
                'enhance_prompt': True,
                'expected_time': 85,
                'content_type': 'image',
                'file_extension': 'png'
            },
            'image7b_high_enhanced': {
                'task': 't2v-1.3B',            # ‚úÖ FIXED: Use t2v-1.3B
                'size': '480*832',
                'sample_steps': 50,
                'sample_guide_scale': 7.5,
                'sample_solver': 'dpm++',     # ‚úÖ FIXED: Changed from unipc to dpm++ for WAN compatibility
                'sample_shift': 5.0,
                'frame_num': 1,
                'enhance_prompt': True,
                'expected_time': 100,
                'content_type': 'image',
                'file_extension': 'png'
            },
            'video7b_fast_enhanced': {
                'task': 't2v-1.3B',            # ‚úÖ FIXED: Use t2v-1.3B
                'size': '480*832',
                'sample_steps': 25,
                'sample_guide_scale': 6.5,
                'sample_solver': 'dpm++',     # ‚úÖ FIXED: Changed from unipc to dpm++ for WAN compatibility
                'sample_shift': 5.0,
                'frame_num': 83,
                'enhance_prompt': True,
                'expected_time': 195,
                'content_type': 'video',
                'file_extension': 'mp4'
            },
            'video7b_high_enhanced': {
                'task': 't2v-1.3B',            # ‚úÖ FIXED: Use t2v-1.3B
                'size': '480*832',
                'sample_steps': 50,
                'sample_guide_scale': 7.5,
                'sample_solver': 'dpm++',     # ‚úÖ FIXED: Changed from unipc to dpm++ for WAN compatibility
                'sample_shift': 5.0,
                'frame_num': 83,
                'enhance_prompt': True,
                'expected_time': 240,
                'content_type': 'video',
                'file_extension': 'mp4'
            }
        }
        
        print("üé¨ Enhanced OurVidz WAN Worker - 1.3B MODEL + QWEN BASE ONLY")
        print("üîß CRITICAL FIX: Using correct t2v-1.3B task for WAN 1.3B model")
        print("üîß REFERENCE SUPPORT: All 5 reference modes (none, single, start, end, both)")
        print("üîß REFERENCE STRENGTH: Adjust guidance scale based on reference strength (0.1-1.0)")
        print("üîß PARAMETER FIX: Consistent parameter names (job_id, assets) with edge function")
        print("üîß CLEANUP: Removed Qwen Instruct model - Base model enhancement only")
        print(f"üìã Supporting ALL 8 job types with 1.3B tasks: {list(self.job_configs.keys())}")
        print(f"üìÅ WAN 1.3B Model Path: {self.model_path}")
        print(f"ü§ñ Qwen Base Model Path: {self.qwen_model_path}")
        print("üìä Status: Fixed for 1.3B model + Reference Frames + Base model enhancement ‚úÖ")
        self.log_gpu_memory()

    def adjust_guidance_for_reference_strength(self, base_guide_scale, reference_strength):
        """
        Adjust sample_guide_scale based on reference strength to control reference influence
        
        Args:
            base_guide_scale (float): Base guidance scale from job config
            reference_strength (float): Reference strength (0.1-1.0)
            
        Returns:
            float: Adjusted guidance scale
        """
        if reference_strength is None:
            return base_guide_scale
            
        # Reference strength affects how much the reference frame influences generation
        # Higher reference strength = higher guidance scale = stronger reference influence
        # Base range: 6.5-7.5, adjusted range: 5.0-9.0
        
        # Calculate adjustment factor
        # 0.1 strength = minimal influence (5.0 guidance)
        # 1.0 strength = maximum influence (9.0 guidance)
        min_guidance = 5.0
        max_guidance = 9.0
        
        # Linear interpolation between min and max guidance
        adjusted_guidance = min_guidance + (max_guidance - min_guidance) * reference_strength
        
        print(f"üéØ Reference strength adjustment:")
        print(f"   Base guidance scale: {base_guide_scale}")
        print(f"   Reference strength: {reference_strength}")
        print(f"   Adjusted guidance scale: {adjusted_guidance:.2f}")
        
        return adjusted_guidance

    def download_image_from_url(self, image_url):
        """Download image from URL and return PIL Image object"""
        try:
            print(f"üì• Downloading reference image from: {image_url}")
            response = requests.get(image_url, timeout=30)
            response.raise_for_status()
            
            # Convert to PIL Image
            image = Image.open(io.BytesIO(response.content))
            
            # Convert to RGB if necessary
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            print(f"‚úÖ Reference image downloaded: {image.size}")
            return image
            
        except Exception as e:
            print(f"‚ùå Failed to download reference image: {e}")
            raise

    def preprocess_reference_image(self, image, target_size=(480, 832)):
        """Preprocess reference image for WAN video generation"""
        try:
            # Resize image to target size while maintaining aspect ratio
            image.thumbnail(target_size, Image.Resampling.LANCZOS)
            
            # Create new image with target size and paste resized image
            new_image = Image.new('RGB', target_size, (0, 0, 0))
            
            # Center the image
            x = (target_size[0] - image.width) // 2
            y = (target_size[1] - image.height) // 2
            new_image.paste(image, (x, y))
            
            print(f"‚úÖ Reference image preprocessed to {target_size}")
            return new_image
            
        except Exception as e:
            print(f"‚ùå Failed to preprocess reference image: {e}")
            raise

    def save_reference_image(self, image, filename):
        """Save reference image to temporary file for WAN processing"""
        try:
            temp_path = f"/tmp/{filename}"
            image.save(temp_path, "PNG", quality=95, optimize=True)
            print(f"üíæ Reference image saved: {temp_path}")
            return temp_path
        except Exception as e:
            print(f"‚ùå Failed to save reference image: {e}")
            raise

    def generate_video_with_references(self, prompt, start_reference, end_reference, strength, job_type):
        """Generate video with start and/or end reference frames using FLF2V task"""
        print(f"üé¨ Generating video with reference frames using FLF2V task")
        
        if start_reference and end_reference:
            print("üñºÔ∏è Using both start and end reference frames with FLF2V-14B")
            return self.generate_flf2v_video(prompt, start_reference, end_reference, job_type)
        elif start_reference:
            print("üñºÔ∏è Using start reference frame only with FLF2V-14B")
            return self.generate_flf2v_video(prompt, start_reference, None, job_type)
        elif end_reference:
            print("üñºÔ∏è Using end reference frame only with FLF2V-14B")
            return self.generate_flf2v_video(prompt, None, end_reference, job_type)
        else:
            print("‚ö†Ô∏è No reference frames provided, falling back to T2V task")
            return self.generate_t2v_video(prompt, job_type)

    def generate_video_with_start_end_references(self, prompt, start_ref, end_ref, strength, job_type):
        """Generate video with both start and end reference frames"""
        print("üé¨ Generating video with start and end reference frames")
        
        # Preprocess reference images
        start_processed = self.preprocess_reference_image(start_ref)
        end_processed = self.preprocess_reference_image(end_ref)
        
        # Save reference images to temp files
        timestamp = int(time.time())
        start_ref_path = self.save_reference_image(start_processed, f"wan_start_ref_{timestamp}.png")
        end_ref_path = self.save_reference_image(end_processed, f"wan_end_ref_{timestamp}.png")
        
        try:
            # Generate video with reference frames using WAN
            output_file = self.generate_content_with_references(prompt, job_type, start_ref_path, end_ref_path, strength)
            return output_file
        finally:
            # Cleanup reference files
            try:
                os.unlink(start_ref_path)
                os.unlink(end_ref_path)
            except:
                pass

    def generate_video_with_start_reference(self, prompt, start_ref, strength, job_type):
        """Generate video with start reference frame only"""
        print("üé¨ Generating video with start reference frame")
        
        # Preprocess reference image
        start_processed = self.preprocess_reference_image(start_ref)
        
        # Save reference image to temp file
        timestamp = int(time.time())
        start_ref_path = self.save_reference_image(start_processed, f"wan_start_ref_{timestamp}.png")
        
        try:
            # Generate video with start reference frame
            output_file = self.generate_content_with_references(prompt, job_type, start_ref_path, None, strength)
            return output_file
        finally:
            # Cleanup reference file
            try:
                os.unlink(start_ref_path)
            except:
                pass

    def generate_video_with_end_reference(self, prompt, end_ref, strength, job_type):
        """Generate video with end reference frame only"""
        print("üé¨ Generating video with end reference frame")
        
        # Preprocess reference image
        end_processed = self.preprocess_reference_image(end_ref)
        
        # Save reference image to temp file
        timestamp = int(time.time())
        end_ref_path = self.save_reference_image(end_processed, f"wan_end_ref_{timestamp}.png")
        
        try:
            # Generate video with end reference frame
            output_file = self.generate_content_with_references(prompt, job_type, None, end_ref_path, strength)
            return output_file
        finally:
            # Cleanup reference file
            try:
                os.unlink(end_ref_path)
            except:
                pass

    def generate_standard_video(self, prompt, job_type):
        """Generate standard video without reference frames"""
        print("üé¨ Generating standard video without reference frames")
        return self.generate_content(prompt, job_type)

    def generate_video_with_reference_frame(self, prompt, reference_image, job_type, denoise_strength=0.15):
        """Generate video with single reference frame using WAN 1.3B (I2V-style)"""
        print(f"üé¨ Generating video with single reference frame using WAN 1.3B")
        
        if job_type not in self.job_configs:
            raise Exception(f"Unsupported job type: {job_type}")
            
        config = self.job_configs[job_type].copy()
        
        # Adjust guidance scale based on denoise strength
        base_guide_scale = config['sample_guide_scale']
        # Convert denoise_strength to reference_strength for guidance adjustment
        reference_strength = 1.0 - denoise_strength
        adjusted_guide_scale = self.adjust_guidance_for_reference_strength(base_guide_scale, reference_strength)
        config['sample_guide_scale'] = adjusted_guide_scale
        
        # Create output path with proper extension
        timestamp = int(time.time())
        file_extension = config['file_extension']
        output_filename = f"wan_single_ref_output_{timestamp}.{file_extension}"
        temp_output_path = f"/tmp/{output_filename}"
        
        # Preprocess reference image
        processed_image = self.preprocess_reference_image(reference_image)
        
        # Save reference image to temp file
        ref_filename = f"wan_single_ref_{timestamp}.png"
        ref_path = self.save_reference_image(processed_image, ref_filename)
        
        print(f"üéØ Single Reference Output path: {temp_output_path}")
        print(f"üìÑ Expected file type: {config['content_type']} (.{file_extension})")
        print(f"üîß FRAME COUNT: {config['frame_num']} frames for {config['content_type']}")
        print(f"üñºÔ∏è Single reference image: {ref_path}")
        
        try:
            # Change to WAN code directory
            original_cwd = os.getcwd()
            os.chdir(self.wan_code_path)
            
            # Build WAN command for single reference frame generation
            wan_generate_path = "/workspace/ourvidz-worker/wan_generate.py"
            
            # Use t2v-1.3B task with --image parameter for single reference frame
            cmd = [
                "python", wan_generate_path,
                "--task", "t2v-1.3B",                        # ‚úÖ CORRECT: Use t2v-1.3B with --image for single reference
                "--ckpt_dir", self.model_path,
                "--offload_model", "True",
                "--size", config['size'],
                "--sample_steps", str(config['sample_steps']),
                "--sample_guide_scale", str(config['sample_guide_scale']),
                "--sample_solver", config.get('sample_solver', 'dpm++'),
                "--sample_shift", str(config.get('sample_shift', 5.0)),
                "--frame_num", str(config['frame_num']),
                "--prompt", prompt,
                "--image", ref_path,                         # ‚úÖ Single reference image for t2v-1.3B generation
                "--save_file", temp_output_path
            ]
            
            print(f"üé¨ WAN 1.3B T2V command with single reference (t2v-1.3B + --image): {' '.join(cmd)}")
            
            # Configure environment
            env = self.setup_environment()
            
            print(f"üé¨ WAN 1.3B T2V generation with single reference (t2v-1.3B + --image): {job_type}")
            print(f"üìù Prompt: {prompt[:100]}...")
            print(f"üîß Config: {config['sample_steps']} steps, {config['frame_num']} frames, {config['size']}")
            print(f"üíæ Output: {temp_output_path}")
            print(f"üìÅ Working dir: {self.wan_code_path}")
            
            # Execute single reference generation
            generation_start = time.time()
            timeout_seconds = 500 if config['content_type'] == 'video' else 180
            
            print(f"‚è∞ Starting WAN 1.3B T2V subprocess (t2v-1.3B + --image) with {timeout_seconds}s timeout")
            print(f"üöÄ T2V generation with single reference started at {time.strftime('%H:%M:%S')}")

            try:
                result = subprocess.run(
                    cmd,
                    cwd=self.wan_code_path,
                    env=env,
                    capture_output=True,
                    text=True,
                    timeout=timeout_seconds
                )
                
                generation_time = time.time() - generation_start
                os.chdir(original_cwd)
                
                print(f"‚úÖ WAN 1.3B T2V subprocess (t2v-1.3B + --image) completed in {generation_time:.1f}s")
                print(f"üìÑ Return code: {result.returncode}")
                
                # Enhanced output analysis
                if result.stdout:
                    stdout_lines = result.stdout.strip().split('\n')
                    print(f"üìÑ STDOUT ({len(stdout_lines)} lines):")
                    for line in stdout_lines[-10:]:
                        print(f"   [OUT] {line}")
                
                if result.stderr:
                    stderr_lines = result.stderr.strip().split('\n')
                    print(f"üìÑ STDERR ({len(stderr_lines)} lines):")
                    for line in stderr_lines[-10:]:
                        print(f"   [ERR] {line}")
                
                # Validate output
                if result.returncode == 0:
                    print(f"üîç Checking T2V output file (t2v-1.3B + --image): {temp_output_path}")
                    
                    if os.path.exists(temp_output_path):
                        file_size = os.path.getsize(temp_output_path)
                        print(f"‚úÖ T2V output file found (t2v-1.3B + --image): {file_size / 1024**2:.2f}MB")
                        
                        is_valid, validation_msg = self.validate_output_file(temp_output_path, config['content_type'])
                        if is_valid:
                            print(f"‚úÖ T2V file validation passed (t2v-1.3B + --image): {validation_msg}")
                            return temp_output_path
                        else:
                            print(f"‚ùå T2V file validation failed (t2v-1.3B + --image): {validation_msg}")
                            raise Exception(f"T2V generated file validation failed: {validation_msg}")
                    else:
                        print(f"‚ùå T2V output file not found (t2v-1.3B + --image): {temp_output_path}")
                        raise Exception("No valid T2V output file generated")
                        
                else:
                    print(f"‚ùå T2V failed with return code (t2v-1.3B + --image): {result.returncode}")
                    error_details = []
                    if result.stderr:
                        error_details.append(f"STDERR: {result.stderr[-300:]}")
                    if result.stdout:
                        error_details.append(f"STDOUT: {result.stdout[-300:]}")
                    
                    error_message = " | ".join(error_details) if error_details else "No error output captured"
                    raise Exception(f"T2V generation failed (t2v-1.3B + --image, code {result.returncode}): {error_message}")
                    
            except subprocess.TimeoutExpired:
                os.chdir(original_cwd)
                print(f"‚ùå T2V generation timed out after {timeout_seconds}s (t2v-1.3B + --image)")
                raise Exception(f"T2V generation timed out after {timeout_seconds} seconds (t2v-1.3B + --image)")
                
            except Exception as e:
                os.chdir(original_cwd)
                print(f"‚ùå T2V subprocess error (t2v-1.3B + --image): {e}")
                raise
                
        except Exception as e:
            if os.getcwd() != original_cwd:
                os.chdir(original_cwd)
            print(f"‚ùå T2V generation error (t2v-1.3B + --image): {e}")
            raise
        finally:
            # Cleanup reference file
            try:
                os.unlink(ref_path)
            except:
                pass

    def generate_video_with_start_frame(self, prompt, start_reference_image, job_type, denoise_strength=0.15):
        """Generate video with start frame reference using WAN 1.3B"""
        print(f"üé¨ Generating video with start frame reference using WAN 1.3B")
        
        if job_type not in self.job_configs:
            raise Exception(f"Unsupported job type: {job_type}")
            
        config = self.job_configs[job_type].copy()
        
        # Adjust guidance scale based on denoise strength
        base_guide_scale = config['sample_guide_scale']
        # Convert denoise_strength to reference_strength for guidance adjustment
        reference_strength = 1.0 - denoise_strength
        adjusted_guide_scale = self.adjust_guidance_for_reference_strength(base_guide_scale, reference_strength)
        config['sample_guide_scale'] = adjusted_guide_scale
        
        # Create output path with proper extension
        timestamp = int(time.time())
        file_extension = config['file_extension']
        output_filename = f"wan_start_frame_output_{timestamp}.{file_extension}"
        temp_output_path = f"/tmp/{output_filename}"
        
        # Preprocess reference image
        processed_image = self.preprocess_reference_image(start_reference_image)
        
        # Save reference image to temp file
        ref_filename = f"wan_start_frame_{timestamp}.png"
        ref_path = self.save_reference_image(processed_image, ref_filename)
        
        print(f"üéØ Start Frame Output path: {temp_output_path}")
        print(f"üìÑ Expected file type: {config['content_type']} (.{file_extension})")
        print(f"üîß FRAME COUNT: {config['frame_num']} frames for {config['content_type']}")
        print(f"üñºÔ∏è Start frame reference: {ref_path}")
        
        try:
            # Change to WAN code directory
            original_cwd = os.getcwd()
            os.chdir(self.wan_code_path)
            
            # Build WAN command for start frame reference generation
            wan_generate_path = "/workspace/ourvidz-worker/wan_generate.py"
            
            # Use t2v-1.3B task with --first_frame parameter for start frame reference
            cmd = [
                "python", wan_generate_path,
                "--task", "t2v-1.3B",                        # ‚úÖ CORRECT: Use t2v-1.3B with --first_frame for start frame
                "--ckpt_dir", self.model_path,
                "--offload_model", "True",
                "--size", config['size'],
                "--sample_steps", str(config['sample_steps']),
                "--sample_guide_scale", str(config['sample_guide_scale']),
                "--sample_solver", config.get('sample_solver', 'dpm++'),
                "--sample_shift", str(config.get('sample_shift', 5.0)),
                "--frame_num", str(config['frame_num']),
                "--prompt", prompt,
                "--first_frame", ref_path,                   # ‚úÖ Start frame reference for t2v-1.3B generation
                "--save_file", temp_output_path
            ]
            
            print(f"üé¨ WAN 1.3B T2V command with start frame (t2v-1.3B + --first_frame): {' '.join(cmd)}")
            
            # Configure environment
            env = self.setup_environment()
            
            print(f"üé¨ WAN 1.3B T2V generation with start frame (t2v-1.3B + --first_frame): {job_type}")
            print(f"üìù Prompt: {prompt[:100]}...")
            print(f"üîß Config: {config['sample_steps']} steps, {config['frame_num']} frames, {config['size']}")
            print(f"üíæ Output: {temp_output_path}")
            print(f"üìÅ Working dir: {self.wan_code_path}")
            
            # Execute start frame generation
            generation_start = time.time()
            timeout_seconds = 500 if config['content_type'] == 'video' else 180
            
            print(f"‚è∞ Starting WAN 1.3B T2V subprocess (t2v-1.3B + --first_frame) with {timeout_seconds}s timeout")
            print(f"üöÄ T2V generation with start frame started at {time.strftime('%H:%M:%S')}")

            try:
                result = subprocess.run(
                    cmd,
                    cwd=self.wan_code_path,
                    env=env,
                    capture_output=True,
                    text=True,
                    timeout=timeout_seconds
                )
                
                generation_time = time.time() - generation_start
                os.chdir(original_cwd)
                
                print(f"‚úÖ WAN 1.3B T2V subprocess (t2v-1.3B + --first_frame) completed in {generation_time:.1f}s")
                print(f"üìÑ Return code: {result.returncode}")
                
                # Enhanced output analysis
                if result.stdout:
                    stdout_lines = result.stdout.strip().split('\n')
                    print(f"üìÑ STDOUT ({len(stdout_lines)} lines):")
                    for line in stdout_lines[-10:]:
                        print(f"   [OUT] {line}")
                
                if result.stderr:
                    stderr_lines = result.stderr.strip().split('\n')
                    print(f"üìÑ STDERR ({len(stderr_lines)} lines):")
                    for line in stderr_lines[-10:]:
                        print(f"   [ERR] {line}")
                
                # Validate output
                if result.returncode == 0:
                    print(f"üîç Checking T2V output file (t2v-1.3B + --first_frame): {temp_output_path}")
                    
                    if os.path.exists(temp_output_path):
                        file_size = os.path.getsize(temp_output_path)
                        print(f"‚úÖ T2V output file found (t2v-1.3B + --first_frame): {file_size / 1024**2:.2f}MB")
                        
                        is_valid, validation_msg = self.validate_output_file(temp_output_path, config['content_type'])
                        if is_valid:
                            print(f"‚úÖ T2V file validation passed (t2v-1.3B + --first_frame): {validation_msg}")
                            return temp_output_path
                        else:
                            print(f"‚ùå T2V file validation failed (t2v-1.3B + --first_frame): {validation_msg}")
                            raise Exception(f"T2V generated file validation failed: {validation_msg}")
                    else:
                        print(f"‚ùå T2V output file not found (t2v-1.3B + --first_frame): {temp_output_path}")
                        raise Exception("No valid T2V output file generated")
                        
                else:
                    print(f"‚ùå T2V failed with return code (t2v-1.3B + --first_frame): {result.returncode}")
                    error_details = []
                    if result.stderr:
                        error_details.append(f"STDERR: {result.stderr[-300:]}")
                    if result.stdout:
                        error_details.append(f"STDOUT: {result.stdout[-300:]}")
                    
                    error_message = " | ".join(error_details) if error_details else "No error output captured"
                    raise Exception(f"T2V generation failed (t2v-1.3B + --first_frame, code {result.returncode}): {error_message}")
                    
            except subprocess.TimeoutExpired:
                os.chdir(original_cwd)
                print(f"‚ùå T2V generation timed out after {timeout_seconds}s (t2v-1.3B + --first_frame)")
                raise Exception(f"T2V generation timed out after {timeout_seconds} seconds (t2v-1.3B + --first_frame)")
                
            except Exception as e:
                os.chdir(original_cwd)
                print(f"‚ùå T2V subprocess error (t2v-1.3B + --first_frame): {e}")
                raise
                
        except Exception as e:
            if os.getcwd() != original_cwd:
                os.chdir(original_cwd)
            print(f"‚ùå T2V generation error (t2v-1.3B + --first_frame): {e}")
            raise
        finally:
            # Cleanup reference file
            try:
                os.unlink(ref_path)
            except:
                pass

    def generate_video_with_end_frame(self, prompt, end_reference_image, job_type, denoise_strength=0.15):
        """Generate video with end frame reference using WAN 1.3B"""
        print(f"üé¨ Generating video with end frame reference using WAN 1.3B")
        
        if job_type not in self.job_configs:
            raise Exception(f"Unsupported job type: {job_type}")
            
        config = self.job_configs[job_type].copy()
        
        # Adjust guidance scale based on denoise strength
        base_guide_scale = config['sample_guide_scale']
        # Convert denoise_strength to reference_strength for guidance adjustment
        reference_strength = 1.0 - denoise_strength
        adjusted_guide_scale = self.adjust_guidance_for_reference_strength(base_guide_scale, reference_strength)
        config['sample_guide_scale'] = adjusted_guide_scale
        
        # Create output path with proper extension
        timestamp = int(time.time())
        file_extension = config['file_extension']
        output_filename = f"wan_end_frame_output_{timestamp}.{file_extension}"
        temp_output_path = f"/tmp/{output_filename}"
        
        # Preprocess reference image
        processed_image = self.preprocess_reference_image(end_reference_image)
        
        # Save reference image to temp file
        ref_filename = f"wan_end_frame_{timestamp}.png"
        ref_path = self.save_reference_image(processed_image, ref_filename)
        
        print(f"üéØ End Frame Output path: {temp_output_path}")
        print(f"üìÑ Expected file type: {config['content_type']} (.{file_extension})")
        print(f"üîß FRAME COUNT: {config['frame_num']} frames for {config['content_type']}")
        print(f"üñºÔ∏è End frame reference: {ref_path}")
        
        try:
            # Change to WAN code directory
            original_cwd = os.getcwd()
            os.chdir(self.wan_code_path)
            
            # Build WAN command for end frame reference generation
            wan_generate_path = "/workspace/ourvidz-worker/wan_generate.py"
            
            # Use t2v-1.3B task with --last_frame parameter for end frame reference
            cmd = [
                "python", wan_generate_path,
                "--task", "t2v-1.3B",                        # ‚úÖ CORRECT: Use t2v-1.3B with --last_frame for end frame
                "--ckpt_dir", self.model_path,
                "--offload_model", "True",
                "--size", config['size'],
                "--sample_steps", str(config['sample_steps']),
                "--sample_guide_scale", str(config['sample_guide_scale']),
                "--sample_solver", config.get('sample_solver', 'dpm++'),
                "--sample_shift", str(config.get('sample_shift', 5.0)),
                "--frame_num", str(config['frame_num']),
                "--prompt", prompt,
                "--last_frame", ref_path,                    # ‚úÖ End frame reference for t2v-1.3B generation
                "--save_file", temp_output_path
            ]
            
            print(f"üé¨ WAN 1.3B T2V command with end frame (t2v-1.3B + --last_frame): {' '.join(cmd)}")
            
            # Configure environment
            env = self.setup_environment()
            
            print(f"üé¨ WAN 1.3B T2V generation with end frame (t2v-1.3B + --last_frame): {job_type}")
            print(f"üìù Prompt: {prompt[:100]}...")
            print(f"üîß Config: {config['sample_steps']} steps, {config['frame_num']} frames, {config['size']}")
            print(f"üíæ Output: {temp_output_path}")
            print(f"üìÅ Working dir: {self.wan_code_path}")
            
            # Execute end frame generation
            generation_start = time.time()
            timeout_seconds = 500 if config['content_type'] == 'video' else 180
            
            print(f"‚è∞ Starting WAN 1.3B T2V subprocess (t2v-1.3B + --last_frame) with {timeout_seconds}s timeout")
            print(f"üöÄ T2V generation with end frame started at {time.strftime('%H:%M:%S')}")

            try:
                result = subprocess.run(
                    cmd,
                    cwd=self.wan_code_path,
                    env=env,
                    capture_output=True,
                    text=True,
                    timeout=timeout_seconds
                )
                
                generation_time = time.time() - generation_start
                os.chdir(original_cwd)
                
                print(f"‚úÖ WAN 1.3B T2V subprocess (t2v-1.3B + --last_frame) completed in {generation_time:.1f}s")
                print(f"üìÑ Return code: {result.returncode}")
                
                # Enhanced output analysis
                if result.stdout:
                    stdout_lines = result.stdout.strip().split('\n')
                    print(f"üìÑ STDOUT ({len(stdout_lines)} lines):")
                    for line in stdout_lines[-10:]:
                        print(f"   [OUT] {line}")
                
                if result.stderr:
                    stderr_lines = result.stderr.strip().split('\n')
                    print(f"üìÑ STDERR ({len(stderr_lines)} lines):")
                    for line in stderr_lines[-10:]:
                        print(f"   [ERR] {line}")
                
                # Validate output
                if result.returncode == 0:
                    print(f"üîç Checking T2V output file (t2v-1.3B + --last_frame): {temp_output_path}")
                    
                    if os.path.exists(temp_output_path):
                        file_size = os.path.getsize(temp_output_path)
                        print(f"‚úÖ T2V output file found (t2v-1.3B + --last_frame): {file_size / 1024**2:.2f}MB")
                        
                        is_valid, validation_msg = self.validate_output_file(temp_output_path, config['content_type'])
                        if is_valid:
                            print(f"‚úÖ T2V file validation passed (t2v-1.3B + --last_frame): {validation_msg}")
                            return temp_output_path
                        else:
                            print(f"‚ùå T2V file validation failed (t2v-1.3B + --last_frame): {validation_msg}")
                            raise Exception(f"T2V generated file validation failed: {validation_msg}")
                    else:
                        print(f"‚ùå T2V output file not found (t2v-1.3B + --last_frame): {temp_output_path}")
                        raise Exception("No valid T2V output file generated")
                        
                else:
                    print(f"‚ùå T2V failed with return code (t2v-1.3B + --last_frame): {result.returncode}")
                    error_details = []
                    if result.stderr:
                        error_details.append(f"STDERR: {result.stderr[-300:]}")
                    if result.stdout:
                        error_details.append(f"STDOUT: {result.stdout[-300:]}")
                    
                    error_message = " | ".join(error_details) if error_details else "No error output captured"
                    raise Exception(f"T2V generation failed (t2v-1.3B + --last_frame, code {result.returncode}): {error_message}")
                    
            except subprocess.TimeoutExpired:
                os.chdir(original_cwd)
                print(f"‚ùå T2V generation timed out after {timeout_seconds}s (t2v-1.3B + --last_frame)")
                raise Exception(f"T2V generation timed out after {timeout_seconds} seconds (t2v-1.3B + --last_frame)")
                
            except Exception as e:
                os.chdir(original_cwd)
                print(f"‚ùå T2V subprocess error (t2v-1.3B + --last_frame): {e}")
                raise
                
        except Exception as e:
            if os.getcwd() != original_cwd:
                os.chdir(original_cwd)
            print(f"‚ùå T2V generation error (t2v-1.3B + --last_frame): {e}")
            raise
        finally:
            # Cleanup reference file
            try:
                os.unlink(ref_path)
            except:
                pass

    def generate_video_with_both_frames(self, prompt, start_reference_image, end_reference_image, job_type, denoise_strength=0.15):
        """Generate video with both start and end frame references using WAN 1.3B"""
        print(f"üé¨ Generating video with both start and end frame references using WAN 1.3B")
        
        if job_type not in self.job_configs:
            raise Exception(f"Unsupported job type: {job_type}")
            
        config = self.job_configs[job_type].copy()
        
        # Adjust guidance scale based on denoise strength
        base_guide_scale = config['sample_guide_scale']
        # Convert denoise_strength to reference_strength for guidance adjustment
        reference_strength = 1.0 - denoise_strength
        adjusted_guide_scale = self.adjust_guidance_for_reference_strength(base_guide_scale, reference_strength)
        config['sample_guide_scale'] = adjusted_guide_scale
        
        # Create output path with proper extension
        timestamp = int(time.time())
        file_extension = config['file_extension']
        output_filename = f"wan_both_frames_output_{timestamp}.{file_extension}"
        temp_output_path = f"/tmp/{output_filename}"
        
        # Preprocess reference images
        processed_start = self.preprocess_reference_image(start_reference_image)
        processed_end = self.preprocess_reference_image(end_reference_image)
        
        # Save reference images to temp files
        start_ref_filename = f"wan_start_frame_{timestamp}.png"
        end_ref_filename = f"wan_end_frame_{timestamp}.png"
        start_ref_path = self.save_reference_image(processed_start, start_ref_filename)
        end_ref_path = self.save_reference_image(processed_end, end_ref_filename)
        
        print(f"üéØ Both Frames Output path: {temp_output_path}")
        print(f"üìÑ Expected file type: {config['content_type']} (.{file_extension})")
        print(f"üîß FRAME COUNT: {config['frame_num']} frames for {config['content_type']}")
        print(f"üñºÔ∏è Start frame reference: {start_ref_path}")
        print(f"üñºÔ∏è End frame reference: {end_ref_path}")
        
        try:
            # Change to WAN code directory
            original_cwd = os.getcwd()
            os.chdir(self.wan_code_path)
            
            # Build WAN command for both frames reference generation
            wan_generate_path = "/workspace/ourvidz-worker/wan_generate.py"
            
            # Use t2v-1.3B task with --first_frame and --last_frame parameters for both frames
            cmd = [
                "python", wan_generate_path,
                "--task", "t2v-1.3B",                        # ‚úÖ CORRECT: Use t2v-1.3B with both frame parameters
                "--ckpt_dir", self.model_path,
                "--offload_model", "True",
                "--size", config['size'],
                "--sample_steps", str(config['sample_steps']),
                "--sample_guide_scale", str(config['sample_guide_scale']),
                "--sample_solver", config.get('sample_solver', 'dpm++'),
                "--sample_shift", str(config.get('sample_shift', 5.0)),
                "--frame_num", str(config['frame_num']),
                "--prompt", prompt,
                "--first_frame", start_ref_path,             # ‚úÖ Start frame reference for t2v-1.3B generation
                "--last_frame", end_ref_path,                # ‚úÖ End frame reference for t2v-1.3B generation
                "--save_file", temp_output_path
            ]
            
            print(f"üé¨ WAN 1.3B T2V command with both frames (t2v-1.3B + --first_frame + --last_frame): {' '.join(cmd)}")
            
            # Configure environment
            env = self.setup_environment()
            
            print(f"üé¨ WAN 1.3B T2V generation with both frames (t2v-1.3B + --first_frame + --last_frame): {job_type}")
            print(f"üìù Prompt: {prompt[:100]}...")
            print(f"üîß Config: {config['sample_steps']} steps, {config['frame_num']} frames, {config['size']}")
            print(f"üíæ Output: {temp_output_path}")
            print(f"üìÅ Working dir: {self.wan_code_path}")
            
            # Execute both frames generation
            generation_start = time.time()
            timeout_seconds = 500 if config['content_type'] == 'video' else 180
            
            print(f"‚è∞ Starting WAN 1.3B T2V subprocess (t2v-1.3B + --first_frame + --last_frame) with {timeout_seconds}s timeout")
            print(f"üöÄ T2V generation with both frames started at {time.strftime('%H:%M:%S')}")

            try:
                result = subprocess.run(
                    cmd,
                    cwd=self.wan_code_path,
                    env=env,
                    capture_output=True,
                    text=True,
                    timeout=timeout_seconds
                )
                
                generation_time = time.time() - generation_start
                os.chdir(original_cwd)
                
                print(f"‚úÖ WAN 1.3B T2V subprocess (t2v-1.3B + --first_frame + --last_frame) completed in {generation_time:.1f}s")
                print(f"üìÑ Return code: {result.returncode}")
                
                # Enhanced output analysis
                if result.stdout:
                    stdout_lines = result.stdout.strip().split('\n')
                    print(f"üìÑ STDOUT ({len(stdout_lines)} lines):")
                    for line in stdout_lines[-10:]:
                        print(f"   [OUT] {line}")
                
                if result.stderr:
                    stderr_lines = result.stderr.strip().split('\n')
                    print(f"üìÑ STDERR ({len(stderr_lines)} lines):")
                    for line in stderr_lines[-10:]:
                        print(f"   [ERR] {line}")
                
                # Validate output
                if result.returncode == 0:
                    print(f"üîç Checking T2V output file (t2v-1.3B + --first_frame + --last_frame): {temp_output_path}")
                    
                    if os.path.exists(temp_output_path):
                        file_size = os.path.getsize(temp_output_path)
                        print(f"‚úÖ T2V output file found (t2v-1.3B + --first_frame + --last_frame): {file_size / 1024**2:.2f}MB")
                        
                        is_valid, validation_msg = self.validate_output_file(temp_output_path, config['content_type'])
                        if is_valid:
                            print(f"‚úÖ T2V file validation passed (t2v-1.3B + --first_frame + --last_frame): {validation_msg}")
                            return temp_output_path
                        else:
                            print(f"‚ùå T2V file validation failed (t2v-1.3B + --first_frame + --last_frame): {validation_msg}")
                            raise Exception(f"T2V generated file validation failed: {validation_msg}")
                    else:
                        print(f"‚ùå T2V output file not found (t2v-1.3B + --first_frame + --last_frame): {temp_output_path}")
                        raise Exception("No valid T2V output file generated")
                        
                else:
                    print(f"‚ùå T2V failed with return code (t2v-1.3B + --first_frame + --last_frame): {result.returncode}")
                    error_details = []
                    if result.stderr:
                        error_details.append(f"STDERR: {result.stderr[-300:]}")
                    if result.stdout:
                        error_details.append(f"STDOUT: {result.stdout[-300:]}")
                    
                    error_message = " | ".join(error_details) if error_details else "No error output captured"
                    raise Exception(f"T2V generation failed (t2v-1.3B + --first_frame + --last_frame, code {result.returncode}): {error_message}")
                    
            except subprocess.TimeoutExpired:
                os.chdir(original_cwd)
                print(f"‚ùå T2V generation timed out after {timeout_seconds}s (t2v-1.3B + --first_frame + --last_frame)")
                raise Exception(f"T2V generation timed out after {timeout_seconds} seconds (t2v-1.3B + --first_frame + --last_frame)")
                
            except Exception as e:
                os.chdir(original_cwd)
                print(f"‚ùå T2V subprocess error (t2v-1.3B + --first_frame + --last_frame): {e}")
                raise
                
        except Exception as e:
            if os.getcwd() != original_cwd:
                os.chdir(original_cwd)
            print(f"‚ùå T2V generation error (t2v-1.3B + --first_frame + --last_frame): {e}")
            raise
        finally:
            # Cleanup reference files
            try:
                os.unlink(start_ref_path)
                os.unlink(end_ref_path)
            except:
                pass

    def generate_standard_content(self, prompt, job_type):
        """Generate standard content without reference frames using WAN 1.3B"""
        print("üé¨ Generating standard content using WAN 1.3B T2V")
        return self.generate_content(prompt, job_type)
        
        if job_type not in self.job_configs:
            raise Exception(f"Unsupported job type: {job_type}")
            
        config = self.job_configs[job_type]
        
        # Create output path with proper extension
        timestamp = int(time.time())
        file_extension = config['file_extension']
        output_filename = f"wan_t2v_output_{timestamp}.{file_extension}"
        temp_output_path = f"/tmp/{output_filename}"
        
        print(f"üéØ T2V Output path: {temp_output_path}")
        print(f"üìÑ Expected file type: {config['content_type']} (.{file_extension})")
        print(f"üîß FRAME COUNT: {config['frame_num']} frames for {config['content_type']}")
        
        try:
            # Change to WAN code directory
            original_cwd = os.getcwd()
            os.chdir(self.wan_code_path)
            
            # Build WAN command for T2V task
            # CRITICAL: Use correct path to wan_generate.py in worker repository
            wan_generate_path = "/workspace/ourvidz-worker/wan_generate.py"
            
            cmd = [
                "python", wan_generate_path,  # ‚úÖ UPDATED: Use full path to wan_generate.py
                "--task", "t2v-14B",  # ‚úÖ UPDATED: Use T2V-14B task for standard generation
                "--ckpt_dir", self.model_path,
                "--offload_model", "True",
                "--size", config['size'],
                "--sample_steps", str(config['sample_steps']),
                "--sample_guide_scale", str(config['sample_guide_scale']),
                "--sample_solver", config.get('sample_solver', 'dpm++'),
                "--sample_shift", str(config.get('sample_shift', 5.0)),
                "--frame_num", str(config['frame_num']),
                "--prompt", prompt,
                "--save_file", temp_output_path
            ]
            
            print(f"üé¨ T2V command: {' '.join(cmd)}")
            
            # Configure environment
            env = self.setup_environment()
            
            print(f"üé¨ T2V generation: {job_type}")
            print(f"üìù Prompt: {prompt[:100]}...")
            print(f"üîß Config: {config['sample_steps']} steps, {config['frame_num']} frames, {config['size']}")
            print(f"üíæ Output: {temp_output_path}")
            print(f"üìÅ Working dir: {self.wan_code_path}")
            
            # Execute T2V generation
            generation_start = time.time()
            timeout_seconds = 500 if config['content_type'] == 'video' else 180
            
            print(f"‚è∞ Starting T2V subprocess with {timeout_seconds}s timeout")
            print(f"üöÄ T2V generation started at {time.strftime('%H:%M:%S')}")

            try:
                result = subprocess.run(
                    cmd,
                    cwd=self.wan_code_path,
                    env=env,
                    capture_output=True,
                    text=True,
                    timeout=timeout_seconds
                )
                
                generation_time = time.time() - generation_start
                os.chdir(original_cwd)
                
                print(f"‚úÖ T2V subprocess completed in {generation_time:.1f}s")
                print(f"üìÑ Return code: {result.returncode}")
                
                # Enhanced output analysis
                if result.stdout:
                    stdout_lines = result.stdout.strip().split('\n')
                    print(f"üìÑ STDOUT ({len(stdout_lines)} lines):")
                    for line in stdout_lines[-10:]:
                        print(f"   [OUT] {line}")
                
                if result.stderr:
                    stderr_lines = result.stderr.strip().split('\n')
                    print(f"üìÑ STDERR ({len(stderr_lines)} lines):")
                    for line in stderr_lines[-10:]:
                        print(f"   [ERR] {line}")
                
                # Validate output
                if result.returncode == 0:
                    print(f"üîç Checking T2V output file: {temp_output_path}")
                    
                    if os.path.exists(temp_output_path):
                        file_size = os.path.getsize(temp_output_path)
                        print(f"‚úÖ T2V output file found: {file_size / 1024**2:.2f}MB")
                        
                        is_valid, validation_msg = self.validate_output_file(temp_output_path, config['content_type'])
                        if is_valid:
                            print(f"‚úÖ T2V file validation passed: {validation_msg}")
                            return temp_output_path
                        else:
                            print(f"‚ùå T2V file validation failed: {validation_msg}")
                            raise Exception(f"T2V generated file validation failed: {validation_msg}")
                    else:
                        print(f"‚ùå T2V output file not found: {temp_output_path}")
                        raise Exception("No valid T2V output file generated")
                        
                else:
                    print(f"‚ùå T2V failed with return code: {result.returncode}")
                    error_details = []
                    if result.stderr:
                        error_details.append(f"STDERR: {result.stderr[-300:]}")
                    if result.stdout:
                        error_details.append(f"STDOUT: {result.stdout[-300:]}")
                    
                    error_message = " | ".join(error_details) if error_details else "No error output captured"
                    raise Exception(f"T2V generation failed (code {result.returncode}): {error_message}")
                    
            except subprocess.TimeoutExpired:
                os.chdir(original_cwd)
                print(f"‚ùå T2V generation timed out after {timeout_seconds}s")
                raise Exception(f"T2V generation timed out after {timeout_seconds} seconds")
                
            except Exception as e:
                os.chdir(original_cwd)
                print(f"‚ùå T2V subprocess error: {e}")
                raise
                
        except Exception as e:
            if os.getcwd() != original_cwd:
                os.chdir(original_cwd)
            print(f"‚ùå T2V generation error: {e}")
            raise

    def generate_content_with_references(self, prompt, job_type, start_ref_path=None, end_ref_path=None, strength=0.5):
        """Generate content with reference frames using WAN command line"""
        if job_type not in self.job_configs:
            raise Exception(f"Unsupported job type: {job_type}")
            
        config = self.job_configs[job_type]
        
        # Create output path with proper extension
        timestamp = int(time.time())
        file_extension = config['file_extension']
        output_filename = f"wan_output_{timestamp}.{file_extension}"
        temp_output_path = f"/tmp/{output_filename}"
        
        print(f"üéØ Output path: {temp_output_path}")
        print(f"üìÑ Expected file type: {config['content_type']} (.{file_extension})")
        print(f"üîß FRAME COUNT: {config['frame_num']} frames for {config['content_type']}")
        
        try:
            # Change to WAN code directory
            original_cwd = os.getcwd()
            os.chdir(self.wan_code_path)
            
            # Build WAN command with FLF2V task for reference frames
            # CRITICAL: Use correct path to wan_generate.py in worker repository
            wan_generate_path = "/workspace/ourvidz-worker/wan_generate.py"
            
            cmd = [
                "python", wan_generate_path,  # ‚úÖ UPDATED: Use full path to wan_generate.py
                "--task", "flf2v-14B",  # ‚úÖ UPDATED: Use FLF2V task for reference frames
                "--ckpt_dir", self.model_path,
                "--offload_model", "True",
                "--size", config['size'],
                "--sample_steps", str(config['sample_steps']),
                "--sample_guide_scale", str(config['sample_guide_scale']),
                "--sample_solver", config.get('sample_solver', 'dpm++'),
                "--sample_shift", str(config.get('sample_shift', 5.0)),
                "--frame_num", str(config['frame_num']),
                "--prompt", prompt,
                "--save_file", temp_output_path
            ]
            
            # Add reference frame parameters for FLF2V task
            if start_ref_path:
                cmd.extend(["--first_frame", start_ref_path])  # ‚úÖ UPDATED: Use --first_frame instead of --start_frame
                print(f"üñºÔ∏è Start reference frame: {start_ref_path}")
            
            if end_ref_path:
                cmd.extend(["--last_frame", end_ref_path])  # ‚úÖ UPDATED: Use --last_frame instead of --end_frame
                print(f"üñºÔ∏è End reference frame: {end_ref_path}")
            
            # ‚úÖ REMOVED: --reference_strength parameter (not needed for FLF2V)
            print(f"üé¨ FLF2V command: {' '.join(cmd)}")
            
            # Configure environment
            env = self.setup_environment()
            
            print(f"üé¨ FLF2V generation: {job_type}")
            print(f"üìù Prompt: {prompt[:100]}...")
            print(f"üîß Config: {config['sample_steps']} steps, {config['frame_num']} frames, {config['size']}")
            print(f"üíæ Output: {temp_output_path}")
            print(f"üìÅ Working dir: {self.wan_code_path}")
            
            # Execute FLF2V generation
            generation_start = time.time()
            timeout_seconds = 500 if config['content_type'] == 'video' else 180
            
            print(f"‚è∞ Starting FLF2V subprocess with {timeout_seconds}s timeout")
            print(f"üöÄ FLF2V generation started at {time.strftime('%H:%M:%S')}")

            try:
                result = subprocess.run(
                    cmd,
                    cwd=self.wan_code_path,
                    env=env,
                    capture_output=True,
                    text=True,
                    timeout=timeout_seconds
                )
                
                generation_time = time.time() - generation_start
                os.chdir(original_cwd)
                
                print(f"‚úÖ FLF2V subprocess completed in {generation_time:.1f}s")
                print(f"üìÑ Return code: {result.returncode}")
                
                # Enhanced output analysis
                if result.stdout:
                    stdout_lines = result.stdout.strip().split('\n')
                    print(f"üìÑ STDOUT ({len(stdout_lines)} lines):")
                    for line in stdout_lines[-10:]:
                        print(f"   [OUT] {line}")
                
                if result.stderr:
                    stderr_lines = result.stderr.strip().split('\n')
                    print(f"üìÑ STDERR ({len(stderr_lines)} lines):")
                    for line in stderr_lines[-10:]:
                        print(f"   [ERR] {line}")
                
                # Validate output
                if result.returncode == 0:
                    print(f"üîç Checking output file: {temp_output_path}")
                    
                    if os.path.exists(temp_output_path):
                        file_size = os.path.getsize(temp_output_path)
                        print(f"‚úÖ Output file found: {file_size / 1024**2:.2f}MB")
                        
                        is_valid, validation_msg = self.validate_output_file(temp_output_path, config['content_type'])
                        if is_valid:
                            print(f"‚úÖ File validation passed: {validation_msg}")
                            return temp_output_path
                        else:
                            print(f"‚ùå File validation failed: {validation_msg}")
                            raise Exception(f"Generated file validation failed: {validation_msg}")
                    else:
                        print(f"‚ùå Output file not found: {temp_output_path}")
                        raise Exception("No valid output file generated")
                        
                else:
                    print(f"‚ùå FLF2V failed with return code: {result.returncode}")
                    error_details = []
                    if result.stderr:
                        error_details.append(f"STDERR: {result.stderr[-300:]}")
                    if result.stdout:
                        error_details.append(f"STDOUT: {result.stdout[-300:]}")
                    
                    error_message = " | ".join(error_details) if error_details else "No error output captured"
                    raise Exception(f"FLF2V generation failed (code {result.returncode}): {error_message}")
                    
            except subprocess.TimeoutExpired:
                os.chdir(original_cwd)
                print(f"‚ùå FLF2V generation timed out after {timeout_seconds}s")
                raise Exception(f"FLF2V generation timed out after {timeout_seconds} seconds")
                
            except Exception as e:
                os.chdir(original_cwd)
                print(f"‚ùå FLF2V subprocess error: {e}")
                raise
                
        except Exception as e:
            if os.getcwd() != original_cwd:
                os.chdir(original_cwd)
            print(f"‚ùå FLF2V generation error: {e}")
            raise

    def log_gpu_memory(self):
        """Monitor RTX 6000 ADA 48GB VRAM usage"""
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated() / 1024**3
            memory_reserved = torch.cuda.memory_reserved() / 1024**3
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"üî• GPU Memory - Used: {memory_allocated:.2f}GB / {total_memory:.0f}GB")

    def setup_environment(self):
        """Configure environment variables for WAN and Qwen - VERIFIED PATHS"""
        env = os.environ.copy()
        
        # CRITICAL: Add persistent dependencies to Python path
        python_deps_path = '/workspace/python_deps/lib/python3.11/site-packages'
        wan_code_path = '/workspace/Wan2.1'  # Add WAN code directory to Python path
        
        current_pythonpath = env.get('PYTHONPATH', '')
        if current_pythonpath:
            new_pythonpath = f"{wan_code_path}:{python_deps_path}:{current_pythonpath}"
        else:
            new_pythonpath = f"{wan_code_path}:{python_deps_path}"
        
        env.update({
            'CUDA_VISIBLE_DEVICES': '0',
            'TORCH_USE_CUDA_DSA': '1',
            'PYTHONUNBUFFERED': '1',
            'PYTHONPATH': new_pythonpath,
            'HF_HOME': self.hf_cache_path,
            'TRANSFORMERS_CACHE': self.hf_cache_path,
            'HUGGINGFACE_HUB_CACHE': f"{self.hf_cache_path}/hub"
        })
        return env

    def _load_qwen_model_internal(self):
        """Internal method for loading Qwen model - used with timeout wrapper"""
        model_path = self.qwen_model_path
        print(f"üîÑ Loading Qwen 2.5-7B Base model from {model_path}")
        
        # Load tokenizer first
        print("üìù Loading tokenizer...")
        self.qwen_tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
        # Load base model - no safety filters
        print("üß† Loading base model...")
        self.qwen_model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,  # Base models work well with bfloat16
            device_map="auto",
            trust_remote_code=True
        )
        
        # Set pad token for base models (they often don't have one)
        if self.qwen_tokenizer.pad_token is None:
            self.qwen_tokenizer.pad_token = self.qwen_tokenizer.eos_token
        
        return True

    def load_qwen_model(self):
        """Load Qwen 2.5-7B Base model for prompt enhancement with thread-safe timeout protection"""
        if self.qwen_model is None:
            print("ü§ñ Loading Qwen 2.5-7B Base model for prompt enhancement...")
            enhancement_start = time.time()
            
            try:
                # ‚úÖ THREAD-SAFE FIX: Use concurrent.futures instead of signal
                run_with_timeout(self._load_qwen_model_internal, 120)  # 2 minute timeout
                
                load_time = time.time() - enhancement_start
                print(f"‚úÖ Qwen 2.5-7B Base loaded successfully in {load_time:.1f}s")
                print(f"‚úÖ Model type: BASE (no content filtering)")
                self.log_gpu_memory()
                
            except TimeoutException:
                print(f"‚ùå Qwen model loading timed out after 120s")
                self.qwen_model = None
                self.qwen_tokenizer = None
            except Exception as e:
                print(f"‚ùå Failed to load Qwen base model: {e}")
                print(f"‚ùå Full error traceback:")
                import traceback
                traceback.print_exc()
                self.qwen_model = None
                self.qwen_tokenizer = None

    def unload_qwen_model(self):
        """Free Qwen memory for WAN generation"""
        if self.qwen_model is not None:
            print("üóëÔ∏è Unloading Qwen 2.5-7B...")
            del self.qwen_model
            del self.qwen_tokenizer
            self.qwen_model = None
            self.qwen_tokenizer = None
            torch.cuda.empty_cache()
            print("‚úÖ Qwen 2.5-7B unloaded")
            self.log_gpu_memory()











    def _enhance_with_qwen_internal(self, enhancement_prompt, inputs):
        """Internal method for Qwen enhancement - optimized generation parameters"""
        print(f"üß† Generating enhancement with Qwen 2.5-7B...")
        
        # Generate with optimized parameters for speed and quality
        with torch.no_grad():
            output = self.qwen_model.generate(
                **inputs,
                max_new_tokens=150,     # ‚úÖ REDUCED: Faster generation
                do_sample=True,
                temperature=0.7,       # Balanced creativity
                top_p=0.9,            # Good diversity
                top_k=40,             # Prevent repetition
                repetition_penalty=1.1,
                pad_token_id=self.qwen_tokenizer.eos_token_id,
                eos_token_id=self.qwen_tokenizer.eos_token_id,
                early_stopping=True    # ‚úÖ ADDED: Stop early when appropriate
            )
        
        # Decode and clean up the response
        enhanced = self.qwen_tokenizer.decode(output[0], skip_special_tokens=True)
        
        # Extract only the generated portion (remove the input prompt)
        if enhancement_prompt in enhanced:
            enhanced = enhanced.split(enhancement_prompt, 1)[-1].strip()
        
        return enhanced

    def enhance_prompt_with_timeout(self, original_prompt):
        """Enhanced NSFW-optimized prompt generation with Base model - optimized timeouts"""
        enhancement_start = time.time()
        print(f"ü§ñ Enhancing NSFW prompt with Base model ({self.enhancement_timeout}s timeout): {original_prompt[:50]}...")
        
        try:
            # Step 1: Load model (separate timeout, keep loaded)
            model_load_start = time.time()
            self.load_qwen_model()
            
            if self.qwen_model is None:
                print("‚ö†Ô∏è Qwen base model not available, using original prompt")
                return original_prompt
            
            model_load_time = time.time() - model_load_start
            print(f"ü§ñ Model ready in {model_load_time:.1f}s")
            
            # Step 2: Prepare enhancement prompt
            enhancement_prompt = f"""Create a detailed, cinematic prompt for AI video generation optimized for adult content. Focus on:

VISUAL DETAILS: High-quality anatomical accuracy, realistic proportions, natural skin textures, detailed facial features, expressive eyes, natural hair flow, realistic body language.

LIGHTING & ATMOSPHERE: Cinematic lighting, soft shadows, warm tones, intimate atmosphere, professional photography style, natural skin tones, flattering angles.

CAMERA WORK: Close-up shots, intimate framing, smooth camera movements, professional cinematography, dynamic angles that enhance the scene.

ARTISTIC STYLE: Photorealistic quality, high resolution details, natural poses, authentic expressions, realistic interactions, professional adult content standards.

TECHNICAL QUALITY: 4K quality, sharp focus, no artifacts, smooth motion, consistent lighting, professional color grading.

Be specific about poses, expressions, clothing, scene composition, and intimate details. Create vivid, unrestricted descriptions that will produce high-quality adult content with realistic anatomy and smooth motion.

Original request: {original_prompt}

Enhanced prompt:"""

            print(f"üìù Tokenizing enhancement prompt...")
            tokenize_start = time.time()
            inputs = self.qwen_tokenizer(enhancement_prompt, return_tensors="pt", truncation=True, max_length=1024)
            
            # Move inputs to the same device as the model
            if hasattr(self.qwen_model, 'device'):
                inputs = {k: v.to(self.qwen_model.device) for k, v in inputs.items()}
            
            tokenize_time = time.time() - tokenize_start
            print(f"üìù Tokenization completed in {tokenize_time:.1f}s")
            
            # Step 3: Generate with shorter timeout (model already loaded)
            generation_timeout = max(15, self.enhancement_timeout - model_load_time - tokenize_time - 5)  # Reserve 5s for cleanup
            print(f"üß† Starting generation with {generation_timeout:.1f}s timeout...")
            
            generation_start = time.time()
            enhanced = run_with_timeout(
                self._enhance_with_qwen_internal, 
                generation_timeout,  # Dynamic timeout based on time remaining
                enhancement_prompt, 
                inputs
            )
            
            generation_time = time.time() - generation_start
            print(f"üß† Generation completed in {generation_time:.1f}s")
            
            # Step 4: Process result
            if enhanced and len(enhanced.strip()) > 10:
                enhanced = enhanced.strip()
                
                # Remove any repetition or artifacts
                if enhanced.lower().startswith("enhanced prompt:"):
                    enhanced = enhanced[16:].strip()
                
                total_time = time.time() - enhancement_start
                print(f"‚úÖ Qwen enhancement completed in {total_time:.1f}s")
                print(f"üìù Enhanced from {len(original_prompt)} to {len(enhanced)} characters")
                print(f"üîç Enhancement preview: {enhanced[:100]}...")
                
                # ‚úÖ KEY FIX: Don't unload model immediately - keep for future requests
                print("ü§ñ Keeping model loaded for future requests")
                return enhanced
            else:
                print("‚ö†Ô∏è Qwen returned empty/invalid enhancement, using original")
                return original_prompt
                
        except TimeoutException:
            remaining_time = self.enhancement_timeout - (time.time() - enhancement_start)
            print(f"‚è∞ Qwen enhancement timed out with {remaining_time:.1f}s remaining, using original prompt")
            return original_prompt
        except Exception as e:
            print(f"‚ùå Qwen enhancement failed: {e}")
            import traceback
            traceback.print_exc()
            return original_prompt
        # ‚úÖ KEY FIX: Don't unload model in finally block - keep loaded
        # finally:
        #     self.unload_qwen_model()

    def enhance_prompt(self, original_prompt, enhancement_type="base", session_id=None, conversation_context=None):
        """Enhanced prompt with retry logic - BASE MODEL ONLY"""
        print(f"ü§ñ Starting base model enhancement for: {original_prompt[:50]}...")
        
        # Only support base model enhancement for enhanced jobs
        if enhancement_type != "base":
            print(f"‚ö†Ô∏è Enhancement type '{enhancement_type}' not supported by WAN worker, using base model")
        
        for attempt in range(self.max_enhancement_attempts):
            try:
                print(f"üîÑ Enhancement attempt {attempt + 1}/{self.max_enhancement_attempts}")
                
                # Use base model only
                enhanced = self.enhance_prompt_with_timeout(original_prompt)
                
                if enhanced and enhanced.strip() != original_prompt.strip():
                    print(f"‚úÖ Base model enhancement successful on attempt {attempt + 1}")
                    return enhanced
                else:
                    print(f"‚ö†Ô∏è Base model returned original prompt on attempt {attempt + 1}")
                    
            except Exception as e:
                print(f"‚ùå Enhancement attempt {attempt + 1} failed: {e}")
                
                if attempt < self.max_enhancement_attempts - 1:
                    wait_time = 2 ** attempt  # Exponential backoff
                    print(f"‚è∞ Waiting {wait_time}s before retry...")
                    time.sleep(wait_time)
        
        print(f"‚ùå All enhancement attempts failed, using original prompt")
        return original_prompt

    def validate_output_file(self, file_path, expected_content_type):
        """Enhanced file validation with MIME type checking"""
        try:
            print(f"üîç ENHANCED FILE VALIDATION:")
            print(f"   File path: {file_path}")
            print(f"   Expected type: {expected_content_type}")
            
            # Check 1: File exists
            if not os.path.exists(file_path):
                print(f"‚ùå File does not exist: {file_path}")
                return False, "File does not exist"
            
            # Check 2: File size
            file_size = os.path.getsize(file_path)
            print(f"üìÅ File size: {file_size / 1024**2:.2f}MB ({file_size} bytes)")
            
            if file_size == 0:
                print(f"‚ùå File is empty")
                return False, "File is empty"
            
            # Check 3: MIME type detection
            mime_type, _ = mimetypes.guess_type(file_path)
            print(f"üîç Detected MIME type: {mime_type}")
            
            # Check 4: Read file header for validation
            try:
                with open(file_path, 'rb') as f:
                    header = f.read(16)
                    print(f"üîç File header (first 16 bytes): {header.hex()}")
                    
                    # Check if it's actually a text file (common WAN error)
                    if header.startswith(b'Traceback') or header.startswith(b'Error') or header.startswith(b'usage:'):
                        print(f"‚ùå File contains error/help text, not {expected_content_type}")
                        return False, f"File contains text data, not {expected_content_type}"
                    
                    # Check for proper file format headers
                    if expected_content_type == 'video':
                        # MP4 file should start with ftyp box
                        if not (b'ftyp' in header or b'mdat' in header):
                            print(f"‚ùå File doesn't have MP4 header signature")
                            return False, "File is not a valid MP4 video"
                    elif expected_content_type == 'image':
                        # PNG should start with PNG signature
                        png_signature = b'\x89PNG\r\n\x1a\n'
                        if not header.startswith(png_signature):
                            print(f"‚ùå File doesn't have PNG header signature")
                            return False, "File is not a valid PNG image"
            except Exception as e:
                print(f"‚ö†Ô∏è Could not read file header: {e}")
            
            # Check 5: Minimum size requirements for 5-second videos
            if expected_content_type == 'video':
                min_size = 350000  # üîß UPDATED: 350KB minimum for 5-second video (was 500KB for 6s)
            else:
                min_size = 5000   # 5KB for image
                
            if file_size < min_size:
                print(f"‚ùå File too small for {expected_content_type}: {file_size} bytes < {min_size} bytes")
                return False, f"File too small for {expected_content_type} (expected at least {min_size} bytes for 5-second video)"
            
            # Check 6: MIME type validation
            expected_mime = 'video/mp4' if expected_content_type == 'video' else 'image/png'
            if mime_type and mime_type != expected_mime:
                print(f"‚ö†Ô∏è MIME type mismatch: expected {expected_mime}, got {mime_type}")
                # Don't fail on MIME type alone, as it might be detected incorrectly
            
            print(f"‚úÖ ENHANCED VALIDATION PASSED")
            return True, "File validation successful"
            
        except Exception as e:
            print(f"‚ùå Validation error: {e}")
            return False, f"Validation error: {e}"

    def generate_content(self, prompt, job_type):
        """CRITICAL FIX: Generate content with proper WAN command formatting"""
        if job_type not in self.job_configs:
            raise Exception(f"Unsupported job type: {job_type}")
            
        config = self.job_configs[job_type]
        
        # CRITICAL FIX: Create output path with proper extension
        timestamp = int(time.time())
        file_extension = config['file_extension']  # Use explicit extension from config
        output_filename = f"wan_output_{timestamp}.{file_extension}"
        temp_output_path = f"/tmp/{output_filename}"
        
        print(f"üéØ FIXED: Output path with proper extension: {temp_output_path}")
        print(f"üìÑ Expected file type: {config['content_type']} (.{file_extension})")
        print(f"üîß FRAME COUNT FIX: {config['frame_num']} frames for {config['content_type']}")
        if config['content_type'] == 'video':
            duration = config['frame_num'] / 16  # 16fps
            print(f"‚è±Ô∏è Expected video duration: {duration:.1f} seconds (83 frames = 5.2 seconds)")
        
        # CRITICAL FIX: Removed negative prompt (not supported by WAN 2.1)
        print(f"üîß FIXED: No negative prompt (WAN 2.1 doesn't support --negative_prompt)")
        
        try:
            # Change to WAN code directory
            original_cwd = os.getcwd()
            os.chdir(self.wan_code_path)
            
            # ENHANCED: Build WAN command with advanced NSFW-optimized parameters
            # Based on WAN 2.1 research: UniPC sampling, temporal consistency, advanced guidance
            # CRITICAL: Use correct path to wan_generate.py in worker repository
            wan_generate_path = "/workspace/ourvidz-worker/wan_generate.py"
            
            cmd = [
                "python", wan_generate_path,
                "--task", config['task'],                       # ‚úÖ FIXED: Use task from config (t2v-1.3B)
                "--ckpt_dir", self.model_path,
                "--offload_model", "True",
                "--size", config['size'],
                "--sample_steps", str(config['sample_steps']),
                "--sample_guide_scale", str(config['sample_guide_scale']),
                "--sample_solver", config.get('sample_solver', 'dpm++'),
                "--sample_shift", str(config.get('sample_shift', 5.0)),
                "--frame_num", str(config['frame_num']),
                "--prompt", prompt,
                "--save_file", temp_output_path
            ]
            
            # Configure environment
            env = self.setup_environment()
            
            print(f"üé¨ FIXED WAN generation: {job_type}")
            print(f"üìù Prompt: {prompt[:100]}...")
            print(f"üîß Config: {config['sample_steps']} steps, {config['frame_num']} frames, {config['size']}")
            print(f"üíæ Output: {temp_output_path}")
            print(f"üìÅ Working dir: {self.wan_code_path}")
            print(f"üîß FIXED Command: {' '.join(cmd)}")
            
            # Environment validation
            print("üîç Environment validation:")
            print(f"   PYTHONPATH: {env.get('PYTHONPATH', 'NOT SET')}")
            print(f"   CUDA_VISIBLE_DEVICES: {env.get('CUDA_VISIBLE_DEVICES', 'NOT SET')}")
            print(f"   Output dir writable: {os.access('/tmp/', os.W_OK)}")
            
            # Execute WAN generation with enhanced monitoring
            generation_start = time.time()
            # üîß UPDATED: Optimized timeout for 83-frame videos
            timeout_seconds = 500 if config['content_type'] == 'video' else 180  # 8 minutes for videos, 3 for images
            
            print(f"‚è∞ Starting WAN subprocess with {timeout_seconds}s timeout")
            print(f"üöÄ Generation started at {time.strftime('%H:%M:%S')}")

            try:
                result = subprocess.run(
                    cmd,
                    cwd=self.wan_code_path,
                    env=env,
                    capture_output=True,
                    text=True,
                    timeout=timeout_seconds
                )
                
                generation_time = time.time() - generation_start
                os.chdir(original_cwd)  # Restore directory
                
                print(f"‚úÖ WAN subprocess completed in {generation_time:.1f}s")
                print(f"üìÑ Return code: {result.returncode}")
                
                # Enhanced output analysis
                if result.stdout:
                    stdout_lines = result.stdout.strip().split('\n')
                    print(f"üìÑ STDOUT ({len(stdout_lines)} lines):")
                    for line in stdout_lines[-10:]:  # Last 10 lines
                        print(f"   [OUT] {line}")
                
                if result.stderr:
                    stderr_lines = result.stderr.strip().split('\n')
                    print(f"üìÑ STDERR ({len(stderr_lines)} lines):")
                    for line in stderr_lines[-10:]:  # Last 10 lines
                        print(f"   [ERR] {line}")
                
                # CRITICAL: Enhanced success validation
                if result.returncode == 0:
                    print(f"üîç Checking output file: {temp_output_path}")
                    
                    # Check if exact file exists
                    if os.path.exists(temp_output_path):
                        file_size = os.path.getsize(temp_output_path)
                        print(f"‚úÖ Output file found: {file_size / 1024**2:.2f}MB")
                        
                        # Enhanced file validation
                        is_valid, validation_msg = self.validate_output_file(temp_output_path, config['content_type'])
                        if is_valid:
                            print(f"‚úÖ File validation passed: {validation_msg}")
                            return temp_output_path
                        else:
                            print(f"‚ùå File validation failed: {validation_msg}")
                            
                            # Show file content for debugging if it's small (likely error text)
                            if file_size < 10000:  # Less than 10KB
                                try:
                                    with open(temp_output_path, 'r', errors='ignore') as f:
                                        content = f.read(500)  # First 500 chars
                                        print(f"üìÑ File content preview: {content}")
                                except:
                                    pass
                            
                            raise Exception(f"Generated file validation failed: {validation_msg}")
                    else:
                        print(f"‚ùå Output file not found: {temp_output_path}")
                        
                        # Look for any files created in /tmp/
                        tmp_files = glob.glob("/tmp/wan_output_*")
                        print(f"üìÅ Files in /tmp/: {tmp_files}")
                        
                        if tmp_files:
                            # Try to use the most recent file
                            latest_file = max(tmp_files, key=os.path.getctime)
                            print(f"üîÑ Trying latest file: {latest_file}")
                            
                            if os.path.getsize(latest_file) > 0:
                                is_valid, validation_msg = self.validate_output_file(latest_file, config['content_type'])
                                if is_valid:
                                    print(f"‚úÖ Using alternative file: {latest_file}")
                                    return latest_file
                        
                        # Include stdout in error for debugging
                        error_context = f"No valid output file. STDOUT: {result.stdout[-300:] if result.stdout else 'None'}"
                        raise Exception(error_context)
                        
                else:
                    print(f"‚ùå WAN failed with return code: {result.returncode}")
                    
                    # Enhanced error analysis
                    error_details = []
                    if result.stderr:
                        error_details.append(f"STDERR: {result.stderr[-300:]}")
                    if result.stdout:
                        error_details.append(f"STDOUT: {result.stdout[-300:]}")
                    
                    error_message = " | ".join(error_details) if error_details else "No error output captured"
                    raise Exception(f"WAN generation failed (code {result.returncode}): {error_message}")
                    
            except subprocess.TimeoutExpired:
                os.chdir(original_cwd)
                print(f"‚ùå WAN generation timed out after {timeout_seconds}s")
                
                # Cleanup partial files
                for partial_file in glob.glob("/tmp/wan_output_*"):
                    try:
                        size = os.path.getsize(partial_file)
                        print(f"üóëÔ∏è Cleaning partial file: {partial_file} ({size} bytes)")
                        os.unlink(partial_file)
                    except:
                        pass
                
                raise Exception(f"WAN generation timed out after {timeout_seconds} seconds")
                
            except Exception as e:
                os.chdir(original_cwd)
                print(f"‚ùå WAN subprocess error: {e}")
                
                # Cleanup partial files
                for partial_file in glob.glob("/tmp/wan_output_*"):
                    try:
                        os.unlink(partial_file)
                    except:
                        pass
                raise
                
        except Exception as e:
            if os.getcwd() != original_cwd:
                os.chdir(original_cwd)
            print(f"‚ùå WAN generation error: {e}")
            
            # Final cleanup
            for partial_file in glob.glob("/tmp/wan_output_*"):
                try:
                    os.unlink(partial_file)
                except:
                    pass
            raise

    def upload_to_supabase(self, file_path, storage_path):
        """Upload file to Supabase storage with enhanced validation"""
        try:
            # Pre-upload validation
            if not os.path.exists(file_path):
                raise Exception(f"File does not exist: {file_path}")
            
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                raise Exception(f"File is empty: {file_path}")
            
            # Enhanced MIME type checking
            mime_type, _ = mimetypes.guess_type(file_path)
            print(f"üì§ Uploading file:")
            print(f"   Path: {file_path}")
            print(f"   Size: {file_size / 1024**2:.2f}MB")
            print(f"   MIME: {mime_type}")
            print(f"   Storage path: {storage_path}")
            
            # Double-check MIME type by reading file header
            with open(file_path, 'rb') as f:
                header = f.read(16)
                print(f"   Header: {header.hex()}")
                
                # Ensure it's not a text file
                if header.startswith(b'Traceback') or header.startswith(b'usage:') or header.startswith(b'Error'):
                    raise Exception(f"File appears to be error text, not binary content")
            
            # Determine MIME type based on file extension
            file_extension = os.path.splitext(file_path)[1].lower()
            if file_extension == '.mp4':
                mime_type = 'video/mp4'
            elif file_extension == '.png':
                mime_type = 'image/png'
            else:
                # Fallback to detected MIME type
                mime_type, _ = mimetypes.guess_type(file_path)
                if not mime_type:
                    mime_type = 'application/octet-stream'
            
            print(f"üì§ Uploading with explicit MIME type: {mime_type}")
            
            with open(file_path, 'rb') as file:
                response = requests.post(
                    f"{self.supabase_url}/storage/v1/object/{storage_path}",
                    files={'file': (os.path.basename(file_path), file, mime_type)},
                    headers={
                        'Authorization': f"Bearer {self.supabase_service_key}",
                    },
                    timeout=180  # 3 minute upload timeout
                )
            
            if response.status_code == 200:
                # Return only relative path within bucket
                path_parts = storage_path.split('/', 1)
                relative_path = path_parts[1] if len(path_parts) == 2 else storage_path
                print(f"‚úÖ Upload successful: {relative_path}")
                return relative_path
            else:
                error_text = response.text[:500]
                print(f"‚ùå Upload failed: {response.status_code}")
                print(f"üìÑ Error response: {error_text}")
                raise Exception(f"Upload failed: {response.status_code} - {error_text}")
                
        except Exception as e:
            print(f"‚ùå Supabase upload error: {e}")
            raise

    def notify_completion(self, job_id, status, assets=None, error_message=None, metadata=None):
        """CONSISTENT: Notify Supabase with standardized callback parameter names and metadata"""
        try:
            # CONSISTENT: Use standardized callback format across all workers
            callback_data = {
                'job_id': job_id,        # ‚úÖ Standard: job_id (snake_case)
                'status': status,        # ‚úÖ Standard: status field
                'assets': assets if assets else [],  # ‚úÖ Standard: assets array
                'error_message': error_message      # ‚úÖ Standard: error_message field
            }
            
            # Add metadata if provided (for generation details)
            if metadata:
                callback_data['metadata'] = metadata
            
            print(f"üìû Sending CONSISTENT callback for job {job_id}:")
            print(f"   Status: {status}")
            print(f"   Assets count: {len(assets) if assets else 0}")
            print(f"   Error: {error_message}")
            print(f"   Metadata: {metadata}")
            print(f"   Parameters: job_id, status, assets, error_message, metadata (CONSISTENT)")
            
            response = requests.post(
                f"{self.supabase_url}/functions/v1/job-callback",
                json=callback_data,
                headers={
                    'Authorization': f"Bearer {self.supabase_service_key}",
                    'Content-Type': 'application/json'
                },
                timeout=30
            )
            
            if response.status_code == 200:
                print(f"‚úÖ CONSISTENT Callback sent successfully for job {job_id}")
            else:
                print(f"‚ùå Callback failed: {response.status_code} - {response.text}")
                print(f"‚ùå Callback payload was: {callback_data}")
                
        except Exception as e:
            print(f"‚ùå Callback error: {e}")

    def process_job_with_enhanced_diagnostics(self, job_data):
        """CONSISTENT: Enhanced process_job with standardized payload structure"""
        # CONSISTENT: Use standardized field names across all workers
        job_id = job_data['id']           # ‚úÖ Standard: 'id' field
        job_type = job_data['type']       # ‚úÖ Standard: 'type' field
        original_prompt = job_data['prompt']  # ‚úÖ Standard: 'prompt' field
        user_id = job_data['user_id']     # ‚úÖ Standard: 'user_id' field
        
        # Optional fields with defaults
        video_id = job_data.get('video_id', f"video_{int(time.time())}")
        image_id = job_data.get('image_id', f"image_{int(time.time())}")
        config = job_data.get('config', {})
        
        # Extract reference frame parameters from config and metadata (UPDATED API SPEC)
        metadata = job_data.get('metadata', {})
        # ‚úÖ NEW: Check config level first, then metadata level (per API spec)
        # Support all reference frame modes: single, start, end, both
        single_reference_url = config.get('image') or metadata.get('reference_image_url')
        start_reference_url = config.get('first_frame') or metadata.get('start_reference_url')
        end_reference_url = config.get('last_frame') or metadata.get('end_reference_url')
        
        # Handle denoise_strength parameter (new) with fallback to reference_strength (deprecated)
        denoise_strength = metadata.get('denoise_strength')
        if denoise_strength is None:
            # Fallback to deprecated reference_strength
            reference_strength = metadata.get('reference_strength', 0.5)
            denoise_strength = 1.0 - reference_strength  # Convert reference_strength to denoise_strength
            print(f"‚ö†Ô∏è DEPRECATED: Using reference_strength={reference_strength}, converted to denoise_strength={denoise_strength}")
        else:
            print(f"‚úÖ Using denoise_strength: {denoise_strength}")
        
        print(f"üîÑ Processing job {job_id} ({job_type}) with CONSISTENT PARAMETERS")
        print(f"üìù Original prompt: {original_prompt}")
        print(f"üéØ Video ID: {video_id}")
        print(f"üë§ User ID: {user_id}")
        
        # Log reference frame parameters if present (UPDATED API SPEC)
        if single_reference_url or start_reference_url or end_reference_url:
            print(f"üñºÔ∏è Reference frame mode: denoise_strength {denoise_strength}")
            if single_reference_url:
                print(f"üì• Single reference frame URL (image/reference_image_url): {single_reference_url}")
            if start_reference_url:
                print(f"üì• Start reference frame URL (first_frame/start_reference_url): {start_reference_url}")
            if end_reference_url:
                print(f"üì• End reference frame URL (last_frame/end_reference_url): {end_reference_url}")
        
        job_start_time = time.time()
        total_time = 0  # Initialize total_time variable
        
        try:
            if job_type not in self.job_configs:
                available_types = list(self.job_configs.keys())
                raise Exception(f"Unknown job type: {job_type}. Available: {available_types}")
            
            job_config = self.job_configs[job_type]
            print(f"‚úÖ Job type validated: {job_type} (enhance: {job_config['enhance_prompt']})")
            
            # Use config from edge function if available, otherwise use defaults
            final_config = {**job_config, **config}
            print(f"üîß OPTIMIZED FRAME COUNT: {final_config['frame_num']} frames")
            if final_config['content_type'] == 'video':
                # Based on confirmed data: 100 frames = 6 seconds, so 16.67fps effective
                effective_fps = 16.67  # Confirmed from successful 6-second generation
                duration = final_config['frame_num'] / effective_fps
                print(f"‚è±Ô∏è Expected duration: {duration:.1f} seconds (confirmed 16.67fps effective rate)")
            
            # Handle prompt enhancement with chat support
            if final_config['enhance_prompt']:
                print("ü§ñ Starting prompt enhancement with chat support...")
                
                # Check if chat enhancement was requested in metadata
                enhancement_type = metadata.get('enhancement_type', 'base')
                session_id = metadata.get('session_id')
                conversation_context = metadata.get('conversation_context')
                
                enhanced_prompt = self.enhance_prompt(
                    original_prompt, 
                    enhancement_type=enhancement_type,
                    session_id=session_id,
                    conversation_context=conversation_context
                )
                actual_prompt = enhanced_prompt
                
                if enhanced_prompt != original_prompt:
                    print(f"‚úÖ Prompt successfully enhanced ({enhancement_type})")
                    print(f"üìù Length: {len(original_prompt)} ‚Üí {len(enhanced_prompt)} chars")
                else:
                    print(f"‚ö†Ô∏è Using original prompt (enhancement failed or timed out)")
            else:
                print("üìù Using original prompt (no enhancement)")
                actual_prompt = original_prompt
            
            # Handle video generation with comprehensive reference frame support for WAN 1.3B
            if final_config['content_type'] == 'video':
                # Determine reference frame mode and route to appropriate generation function
                if single_reference_url and not start_reference_url and not end_reference_url:
                    # Single reference frame mode (I2V-style)
                    print("üé¨ Starting single reference video generation (t2v-1.3B + --image)...")
                    print(f"üîß SINGLE REFERENCE TASK: {final_config['frame_num']} frames for 5-second videos")
                    
                    try:
                        reference_image = self.download_image_from_url(single_reference_url)
                        print(f"‚úÖ Single reference image loaded successfully")
                        
                        # Generate video with single reference frame
                        output_file = self.generate_video_with_reference_frame(
                            actual_prompt, 
                            reference_image, 
                            job_type,
                            denoise_strength
                        )
                    except Exception as e:
                        print(f"‚ùå Failed to load single reference image: {e}")
                        print(f"üîÑ Falling back to standard generation")
                        output_file = self.generate_standard_content(actual_prompt, job_type)
                        
                elif start_reference_url and end_reference_url:
                    # Both frames mode (start + end)
                    print("üé¨ Starting both frames video generation (t2v-1.3B + --first_frame + --last_frame)...")
                    print(f"üîß BOTH FRAMES TASK: {final_config['frame_num']} frames for 5-second videos")
                    
                    try:
                        start_reference_image = self.download_image_from_url(start_reference_url)
                        end_reference_image = self.download_image_from_url(end_reference_url)
                        print(f"‚úÖ Both reference images loaded successfully")
                        
                        # Generate video with both start and end frames
                        output_file = self.generate_video_with_both_frames(
                            actual_prompt, 
                            start_reference_image, 
                            end_reference_image, 
                            job_type,
                            denoise_strength
                        )
                    except Exception as e:
                        print(f"‚ùå Failed to load both reference images: {e}")
                        print(f"üîÑ Falling back to standard generation")
                        output_file = self.generate_standard_content(actual_prompt, job_type)
                        
                elif start_reference_url and not end_reference_url:
                    # Start frame only mode
                    print("üé¨ Starting start frame video generation (t2v-1.3B + --first_frame)...")
                    print(f"üîß START FRAME TASK: {final_config['frame_num']} frames for 5-second videos")
                    
                    try:
                        start_reference_image = self.download_image_from_url(start_reference_url)
                        print(f"‚úÖ Start reference image loaded successfully")
                        
                        # Generate video with start frame only
                        output_file = self.generate_video_with_start_frame(
                            actual_prompt, 
                            start_reference_image, 
                            job_type,
                            denoise_strength
                        )
                    except Exception as e:
                        print(f"‚ùå Failed to load start reference image: {e}")
                        print(f"üîÑ Falling back to standard generation")
                        output_file = self.generate_standard_content(actual_prompt, job_type)
                        
                elif end_reference_url and not start_reference_url:
                    # End frame only mode
                    print("üé¨ Starting end frame video generation (t2v-1.3B + --last_frame)...")
                    print(f"üîß END FRAME TASK: {final_config['frame_num']} frames for 5-second videos")
                    
                    try:
                        end_reference_image = self.download_image_from_url(end_reference_url)
                        print(f"‚úÖ End reference image loaded successfully")
                        
                        # Generate video with end frame only
                        output_file = self.generate_video_with_end_frame(
                            actual_prompt, 
                            end_reference_image, 
                            job_type,
                            denoise_strength
                        )
                    except Exception as e:
                        print(f"‚ùå Failed to load end reference image: {e}")
                        print(f"üîÑ Falling back to standard generation")
                        output_file = self.generate_standard_content(actual_prompt, job_type)
                        
                else:
                    # Standard generation (no reference frames)
                    print("üé¨ Starting T2V video generation (standard video)...")
                    print(f"üîß T2V TASK: {final_config['frame_num']} frames for 5-second videos")
                    
                    # Generate video with T2V task (standard generation)
                    output_file = self.generate_standard_content(actual_prompt, job_type)
            else:
                print("üé¨ Starting WAN image generation...")
                print(f"üîß IMAGE GENERATION: {final_config['frame_num']} frames")
                
                # Generate image content
                output_file = self.generate_content(actual_prompt, job_type)
            
            if not output_file:
                raise Exception("Content generation failed or produced no output")
            
            # Final file validation before upload
            print(f"üîç Final validation before upload:")
            is_valid, validation_msg = self.validate_output_file(output_file, final_config['content_type'])
            if not is_valid:
                raise Exception(f"Generated file failed final validation: {validation_msg}")
            
            # Upload to workspace-temp bucket
            print(f"üì§ Uploading validated {final_config['content_type']} file to workspace-temp bucket")
            # Pass denoise_strength if reference frames were used
            has_reference_frames = single_reference_url or start_reference_url or end_reference_url
            uploaded_assets = self.upload_video(output_file, job_id, user_id, denoise_strength=denoise_strength if has_reference_frames else None)
            
            # Cleanup temp file
            try:
                os.unlink(output_file)
                print(f"üóëÔ∏è Cleaned up temp file: {output_file}")
            except:
                pass
            
            # Calculate total time
            total_time = time.time() - job_start_time
            
            # Prepare metadata for callback
            callback_metadata = {
                'generation_time': total_time,
                'job_type': job_type,
                'content_type': final_config['content_type'],
                'frame_num': final_config['frame_num'],
                'wan_task': 't2v-1.3B',  # Always t2v-1.3B for WAN 1.3B model
                'reference_mode': self._determine_reference_mode(single_reference_url, start_reference_url, end_reference_url)
            }
            
            # CONSISTENT: Success callback with standardized parameters and metadata
            self.notify_completion(job_id, 'completed', assets=uploaded_assets, metadata=callback_metadata)
            print(f"üéâ Job {job_id} completed successfully in {total_time:.1f}s")
            print(f"üìÅ Output: {len(uploaded_assets)} assets uploaded")
            print(f"‚úÖ File type: {final_config['content_type']}")
            if final_config['content_type'] == 'video':
                effective_fps = 16.67  # Confirmed from successful generation: 100 frames = 6 seconds
                duration = final_config['frame_num'] / effective_fps
                print(f"‚è±Ô∏è Video duration: {duration:.1f} seconds ({final_config['frame_num']} frames at 16.67fps confirmed)")
            
        except Exception as e:
            error_msg = str(e)
            total_time = time.time() - job_start_time
            print(f"‚ùå Job {job_id} failed after {total_time:.1f}s: {error_msg}")
            
            # Cleanup any temp files
            try:
                for temp_file in glob.glob("/tmp/wan_output_*"):
                    os.unlink(temp_file)
            except:
                pass
            
            # Prepare error metadata
            error_metadata = {
                'error_type': type(e).__name__,
                'job_type': job_type,
                'wan_task': 't2v-1.3B',  # Always t2v-1.3B for WAN 1.3B model
                'timestamp': time.time()
            }
            
            # CONSISTENT: Failure callback with standardized parameters and metadata
            self.notify_completion(job_id, 'failed', error_message=error_msg, metadata=error_metadata)

    def _determine_reference_mode(self, single_reference_url, start_reference_url, end_reference_url):
        """Determine the reference frame mode based on provided URLs"""
        if single_reference_url and not start_reference_url and not end_reference_url:
            return 'single'
        elif start_reference_url and end_reference_url:
            return 'both'
        elif start_reference_url and not end_reference_url:
            return 'start'
        elif end_reference_url and not start_reference_url:
            return 'end'
        else:
            return 'none'

    def poll_queue(self):
        """Poll Redis queue for new jobs with non-blocking RPOP (Upstash REST API compatible)"""
        try:
            response = requests.get(
                f"{self.redis_url}/rpop/wan_queue",
                headers={
                    'Authorization': f"Bearer {self.redis_token}"
                },
                timeout=10
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get('result'):
                    job_json = result['result']
                    job_data = json.loads(job_json)
                    return job_data
            
            return None
            
        except requests.exceptions.Timeout:
            return None
        except Exception as e:
            print(f"‚ùå Queue polling error: {e}")
            return None

    def run_with_enhanced_diagnostics(self):
        """Main worker loop for WAN 1.3B model"""
        print("üé¨ Enhanced OurVidz WAN Worker - 1.3B MODEL + REFERENCE FRAMES")
        print("üîß CRITICAL FIX: Using correct t2v-1.3B task for WAN 1.3B model")
        print("üîß REFERENCE SUPPORT: All 5 reference modes (none, single, start, end, both)")
        print("üîß PARAMETER FIX: Consistent callback parameters (job_id, status, assets)")
        print("üìä Status: Fixed for WAN 1.3B + Reference Frame Support ‚úÖ")
        
        print("üîß UPSTASH COMPATIBLE: Using non-blocking RPOP for Redis polling")
        print("üìã Supported job types with 1.3B tasks:")
        for job_type, config in self.job_configs.items():
            enhancement = "‚ú® Enhanced" if config['enhance_prompt'] else "üìù Standard"
            content = "üñºÔ∏è Image" if config['content_type'] == 'image' else "üé¨ Video"
            print(f"  ‚Ä¢ {job_type}: {content} ({config['task']}) {enhancement}")
        print("‚è≥ Waiting for jobs...")
        
        job_count = 0
        consecutive_errors = 0
        max_consecutive_errors = 5
        
        while True:
            try:
                job_data = self.poll_queue()
                if job_data:
                    job_count += 1
                    consecutive_errors = 0
                    print(f"\nüì¨ WAN 1.3B Job #{job_count} received")
                    self.process_job_with_enhanced_diagnostics(job_data)
                    print("=" * 60)
                else:
                    time.sleep(5)
                    
            except KeyboardInterrupt:
                print("üõë Worker stopped by user")
                break
            except Exception as e:
                consecutive_errors += 1
                print(f"‚ùå Worker error #{consecutive_errors}: {e}")
                if consecutive_errors >= max_consecutive_errors:
                    print(f"‚ùå Too many consecutive errors ({consecutive_errors}), shutting down worker")
                    break
                sleep_time = min(30, 5 * consecutive_errors)
                print(f"‚è≥ Waiting {sleep_time}s before retry...")
                time.sleep(sleep_time)

    def upload_to_supabase_storage(self, bucket, path, file_data, content_type='video/mp4'):
        """Upload file data to Supabase storage bucket with correct Content-Type"""
        try:
            supabase_url = os.environ.get('SUPABASE_URL')
            supabase_service_key = os.environ.get('SUPABASE_SERVICE_KEY')
            
            if not supabase_url or not supabase_service_key:
                print("‚ùå Missing Supabase credentials")
                return None
            
            headers = {
                'Authorization': f"Bearer {supabase_service_key}",
                'Content-Type': content_type,  # ‚úÖ Use correct Content-Type for video
                'x-upsert': 'true'
            }
            
            response = requests.post(
                f"{supabase_url}/storage/v1/object/{bucket}/{path}",
                data=file_data,
                headers=headers,
                timeout=180
            )
            
            if response.status_code in [200, 201]:
                return path
            else:
                print(f"‚ùå Upload failed: {response.status_code} - {response.text}")
                return None
                
        except Exception as e:
            print(f"‚ùå Upload error: {e}")
            return None

    def upload_video(self, video_path, job_id, user_id, denoise_strength=None):
        """Upload video to workspace-temp bucket with thumbnail"""
        # Simple path: workspace-temp/{user_id}/{job_id}/0.mp4
        storage_path = f"{user_id}/{job_id}/0.mp4"
        thumbnail_path = f"{user_id}/{job_id}/0.thumb.webp"
        print(f"üì§ Uploading video to workspace-temp/{storage_path}")
        
        # Upload to workspace-temp bucket with correct Content-Type
        with open(video_path, 'rb') as video_file:
            video_data = video_file.read()
            upload_result = self.upload_to_supabase_storage(
                bucket='workspace-temp',
                path=storage_path,
                file_data=video_data,
                content_type='video/mp4'  # ‚úÖ Explicitly set video Content-Type
            )
        
        # Generate and upload thumbnail if video upload was successful
        thumbnail_url = None
        if upload_result:
            thumbnail_bytes = self.generate_video_thumbnail(video_path)
            if thumbnail_bytes:
                thumbnail_upload_result = self.upload_to_supabase_storage(
                    bucket='workspace-temp',
                    path=thumbnail_path,
                    file_data=thumbnail_bytes,
                    content_type='image/webp'
                )
                if thumbnail_upload_result:
                    thumbnail_url = thumbnail_path
                    print(f"‚úÖ Successfully uploaded video thumbnail to workspace-temp/{thumbnail_path}")
                else:
                    print(f"‚ö†Ô∏è Failed to upload video thumbnail to workspace-temp/{thumbnail_path}")
        
        if upload_result:
            print(f"‚úÖ Successfully uploaded video to workspace-temp/{storage_path} (Content-Type: video/mp4)")
            
            # Build metadata with denoise_strength if provided
            metadata = {
                'file_size_bytes': os.path.getsize(video_path),
                'format': 'mp4',
                'duration_seconds': self.get_video_duration(video_path),
                'generation_seed': getattr(self, 'generation_seed', 0),  # Default to 0 if not set
                'asset_index': 0
            }
            
            # Add I2I-specific metadata if provided
            if denoise_strength is not None:
                metadata['denoise_strength'] = denoise_strength
                metadata['pipeline'] = 'img2img'
                metadata['resize_policy'] = 'center_crop'
            
            asset = {
                'type': 'video',
                'url': storage_path,  # ‚úÖ Use 'url' field as expected by edge function
                'metadata': metadata
            }
            
            # Add thumbnail_url if available
            if thumbnail_url:
                asset['thumbnail_url'] = thumbnail_url
            
            return [asset]
        else:
            print(f"‚ùå Failed to upload video to workspace-temp/{storage_path}")
            return []

    def get_video_duration(self, video_path):
        """Get video duration in seconds"""
        try:
            # Simple duration calculation based on frame count and FPS
            # WAN 1.3B generates at ~16.67 FPS
            # For now, return a default duration based on file size estimation
            file_size = os.path.getsize(video_path)
            # Rough estimation: 1MB ‚âà 1 second for MP4
            estimated_duration = file_size / (1024 * 1024)
            return max(1.0, estimated_duration)  # Minimum 1 second
        except Exception as e:
            print(f"‚ö†Ô∏è Could not determine video duration: {e}")
            return 5.0  # Default 5 seconds for WAN videos

    def generate_video_thumbnail(self, video_path, max_size=256):
        """Generate a thumbnail from the mid-frame of the video for better representation"""
        try:
            import cv2
            
            # Open video and get total frame count
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            if total_frames <= 0:
                print(f"‚ùå Could not determine frame count for video: {video_path}")
                cap.release()
                return None
            
            # Read mid-frame for better thumbnail representation
            mid_frame = total_frames // 2
            cap.set(cv2.CAP_PROP_POS_FRAMES, mid_frame)
            ret, frame = cap.read()
            cap.release()
            
            if not ret:
                print(f"‚ùå Could not read first frame from video: {video_path}")
                return None
            
            # Convert BGR to RGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # Convert to PIL Image
            image = Image.fromarray(frame_rgb)
            
            # Calculate new size maintaining aspect ratio
            width, height = image.size
            if width > height:
                new_width = max_size
                new_height = int(height * max_size / width)
            else:
                new_height = max_size
                new_width = int(width * max_size / height)
            
            # Resize image
            thumbnail = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
            
            # Convert to WEBP format
            thumbnail_buffer = io.BytesIO()
            thumbnail.save(thumbnail_buffer, format='WEBP', quality=85, optimize=True)
            thumbnail_buffer.seek(0)
            
            print(f"‚úÖ Generated video thumbnail from mid-frame ({mid_frame}/{total_frames}): {new_width}x{new_height} WEBP")
            return thumbnail_buffer.getvalue()
            
        except Exception as e:
            print(f"‚ùå Video thumbnail generation failed: {e}")
            return None

# Flask server for frontend enhancement API
if FLASK_AVAILABLE:

    @app.route('/', methods=['GET'])
    def root():
        """Root endpoint for basic connectivity testing"""
        return jsonify({
            'service': 'OurVidz WAN Worker',
            'status': 'online',
            'thread_safe_timeouts': True,  # ‚úÖ NEW: Indicate thread-safe fix
            'endpoints': {
                '/': 'GET - This status page',
                '/health': 'GET - Health check', 
                '/enhance': 'POST - Prompt enhancement (thread-safe)'
            }
        })

    @app.route('/enhance', methods=['POST'])
    def enhance_endpoint():
        """Enhanced prompt endpoint - BASE MODEL ONLY"""
        try:
            # API Key Authentication
            auth_header = request.headers.get('Authorization')
            expected_key = os.environ.get('WAN_WORKER_API_KEY', 'default_key_123')
            
            if not auth_header or not auth_header.startswith('Bearer '):
                return jsonify({
                    'success': False,
                    'error': 'Missing or invalid Authorization header'
                }), 401
            
            provided_key = auth_header.replace('Bearer ', '')
            
            if provided_key != expected_key:
                return jsonify({
                    'success': False,
                    'error': 'Invalid API key'
                }), 401
            
            # Validate request
            data = request.json
            if not data or 'prompt' not in data:
                return jsonify({
                    'success': False,
                    'error': 'prompt is required'
                }), 400
            
            original_prompt = data.get('prompt', '')
            model = data.get('model', 'qwen_base')
            
            if not original_prompt.strip():
                return jsonify({
                    'success': False,
                    'error': 'prompt cannot be empty'
                }), 400
            
            print(f"üéØ WAN Worker enhancement request: {original_prompt[:50]}...")
            start_time = time.time()
            
            # Force base model for WAN worker
            if model != 'qwen_base':
                print(f"‚ö†Ô∏è Model '{model}' not supported by WAN worker, using qwen_base")
                model = 'qwen_base'
            
                worker = globals().get('worker_instance')
                if not worker:
                    return jsonify({
                        'success': False,
                        'error': 'Worker not initialized'
                    }), 500
                
            # Optimize timeout based on model loading status
                original_timeout = worker.enhancement_timeout
            if hasattr(worker, 'qwen_model') and worker.qwen_model is not None:
                worker.enhancement_timeout = 30
                print("üöÄ Model already loaded, using 30s timeout")
            else:
                worker.enhancement_timeout = 90
                print("‚è≥ Model needs loading, using 90s timeout")
            
            try:
                # Enhance using base model only
                enhanced_prompt = worker.enhance_prompt(original_prompt, enhancement_type="base")
                processing_time = time.time() - start_time
                
                # Check if enhancement actually happened
                enhancement_applied = enhanced_prompt != original_prompt
                    
                if enhancement_applied:
                    response_data = {
                        'success': True,
                        'enhanced_prompt': enhanced_prompt,
                        'original_prompt': original_prompt,
                        'enhancement_source': 'qwen_base',
                        'processing_time': processing_time,
                        'model': model,
                        'thread_safe': True,
                        'enhancement_applied': enhancement_applied,
                        'model_was_loaded': hasattr(worker, 'qwen_model') and worker.qwen_model is not None,
                        'worker': 'wan',
                        'note': 'Enhanced using Qwen Base model'
                    }
                    print(f"‚úÖ WAN enhancement successful: {len(original_prompt)} ‚Üí {len(enhanced_prompt)} chars")
                    return jsonify(response_data)
                else:
                    print(f"‚ö†Ô∏è WAN enhancement failed, returning original")
                    return jsonify({
                        'success': False,
                        'enhanced_prompt': original_prompt,
                        'original_prompt': original_prompt,
                        'error': 'Enhancement failed, returned original prompt'
                    })
                    
            except Exception as e:
                print(f"‚ùå WAN enhancement failed: {e}")
                return jsonify({
                    'success': False,
                    'error': f'Enhancement failed: {str(e)}',
                    'enhanced_prompt': original_prompt
                }), 500
            finally:
                # Restore original timeout
                worker.enhancement_timeout = original_timeout
                
        except Exception as e:
            print(f"‚ùå WAN enhancement endpoint error: {e}")
            return jsonify({
                'success': False,
                'error': str(e),
                'enhanced_prompt': data.get('prompt', '') if data else ''
            }), 500

    @app.route('/health', methods=['GET'])
    def health_check():
        """Health check endpoint"""
        worker = globals().get('worker_instance')
        return jsonify({
            'status': 'healthy',
            'qwen_loaded': worker and hasattr(worker, 'qwen_model') and worker.qwen_model is not None,
            'timestamp': time.time(),
            'worker_ready': worker is not None,
            'thread_safe_timeouts': True  # ‚úÖ NEW: Indicate thread-safe fix
        })

    # Memory management endpoints
    @app.route('/memory/status', methods=['GET'])
    def memory_status():
        """Memory status endpoint"""
        worker = globals().get('worker_instance')
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / (1024**3)
            total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            available = total - allocated
            
            response = {
                'total_vram': total,
                'allocated_vram': allocated,
                'available_vram': available,
                'qwen_model_loaded': worker and hasattr(worker, 'qwen_model') and worker.qwen_model is not None,
                'wan_model_loaded': worker and hasattr(worker, 'wan_model') and worker.qwen_model is not None,
                'memory_fraction': 0.63,  # WAN worker memory fraction (30GB / 48GB)
                'worker_type': 'wan'
            }
            
            return jsonify(response)
        else:
            return jsonify({'error': 'CUDA not available'}), 500

    @app.route('/memory/unload', methods=['POST'])
    def force_unload():
        """Force unload model (for memory management)"""
        worker = globals().get('worker_instance')
        which = request.args.get('which', 'all')
        
        try:
            if worker:
                if which in ('qwen', 'all'):
                    if hasattr(worker, 'unload_qwen_model'):
                        worker.unload_qwen_model()
                if which in ('wan', 'all'):
                    if hasattr(worker, 'unload_wan_model'):
                        worker.unload_wan_model()
            
            torch.cuda.empty_cache()
            import gc
            gc.collect()
            
            return jsonify({'success': True, 'message': f'WAN models unloaded: {which}'})
        except Exception as e:
            return jsonify({'success': False, 'message': f'Unload failed: {str(e)}'}), 500

    @app.route('/memory/load', methods=['POST'])
    def force_load():
        """Force load model"""
        worker = globals().get('worker_instance')
        which = request.args.get('which', 'qwen')
        
        try:
            if worker:
                if which == 'qwen' and hasattr(worker, 'load_qwen_model'):
                    success = worker.load_qwen_model()
                elif which == 'wan' and hasattr(worker, 'load_wan_model'):
                    success = worker.load_wan_model()
                else:
                    return jsonify({'success': False, 'message': f'Unknown model type: {which}'}), 400
                
                return jsonify({'success': success, 'message': f'{which} model load attempted'})
            else:
                return jsonify({'success': False, 'message': 'Worker not available'}), 500
        except Exception as e:
            return jsonify({'success': False, 'message': f'Load failed: {str(e)}'}), 500

    @app.route('/debug/env', methods=['GET'])
    def debug_env():
        """Debug endpoint to check environment variables"""
        wan_key = os.environ.get('WAN_WORKER_API_KEY')
        return jsonify({
            'wan_worker_api_key_set': bool(wan_key),
            'wan_worker_api_key_value': wan_key[:10] + '...' if wan_key else 'NOT SET',
            'expected_key': wan_key[:10] + '...' if wan_key else 'default_key_123',
            'full_api_key': wan_key if wan_key else 'NOT SET',  # TEMPORARY: Show full key for testing
            'thread_safe_timeouts': True,  # ‚úÖ NEW: Indicate thread-safe fix
            'all_env_vars': {k: v for k, v in os.environ.items() if 'KEY' in k or 'URL' in k or 'TOKEN' in k}  # Show relevant env vars
        })

    def run_flask_server():
        """Run Flask server in a separate thread"""
        try:
            print("üåê Starting Flask server for frontend enhancement on port 7860...")
            print(f"üîß Flask app object: {app}")
            print(f"üîß Flask routes: {[rule.rule for rule in app.url_map.iter_rules()]}")
            
            # ‚úÖ FIXED: Use dynamic URL detection instead of hard-coded URL
            detected_url = detect_runpod_url()
            if detected_url:
                print(f"üåê Public endpoint: {detected_url}/")
            else:
                print("üåê Public endpoint: [URL detection pending]")
            
            print("üöÄ Flask server starting on 0.0.0.0:7860...")
            app.run(host='0.0.0.0', port=7860, debug=False, threaded=True, use_reloader=False)
        except Exception as e:
            print(f"‚ùå Flask server failed to start: {e}")
            import traceback
            traceback.print_exc()
else:
    # Placeholder functions when Flask is not available
    def run_flask_server():
        print("‚ö†Ô∏è Flask server not started - Flask not available")
    
    def enhance_endpoint():
        pass
    
    def health_check():
        pass

if __name__ == "__main__":
    # Environment variable validation
    required_vars = [
        'SUPABASE_URL',
        'SUPABASE_SERVICE_KEY', 
        'UPSTASH_REDIS_REST_URL',
        'UPSTASH_REDIS_REST_TOKEN',
        'WAN_WORKER_API_KEY'  # ‚úÖ ADDED: Required for /enhance endpoint
    ]
    
    # Debug: Check environment variables before validation
    print("üîç Environment variable check:")
    for var in required_vars:
        value = os.getenv(var)
        if value:
            print(f"  ‚úÖ {var}: {'*' * min(len(value), 10)}... (length: {len(value)})")
        else:
            print(f"  ‚ùå {var}: NOT SET")
    
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    if missing_vars:
        print(f"‚ùå Missing environment variables: {', '.join(missing_vars)}")
        exit(1)
    
    # Verify critical paths
    model_path = "/workspace/models/wan2.1-t2v-1.3b"
    qwen_path = "/workspace/models/huggingface_cache/hub/models--Qwen--Qwen2.5-7B/snapshots/d149729398750b98c0af14eb82c78cfe92750796"
    wan_code_path = "/workspace/Wan2.1"
    
    if not os.path.exists(model_path):
        print(f"‚ùå WAN model not found: {model_path}")
        exit(1)
        
    if not os.path.exists(qwen_path):
        print(f"‚ö†Ô∏è Qwen Base model not found: {qwen_path} (enhancement will be disabled)")
        
    if not os.path.exists(wan_code_path):
        print(f"‚ùå WAN code not found: {wan_code_path}")
        exit(1)
    
    print("‚úÖ All paths validated for 1.3B model")
    print("üîß FIXED: Using t2v-1.3B task for WAN 1.3B model")
    print("üñºÔ∏è REFERENCE: All 5 reference modes (none, single, start, end, both)")
    print("‚úÖ THREAD-SAFE: Replaced signal timeouts with concurrent.futures")
    print("üîß CLEANUP: Removed Qwen Instruct model - Base model enhancement only")
    
    try:
        # Initialize worker
        worker = EnhancedWanWorker()
        
        # Make worker available globally for Flask endpoint
        globals()['worker_instance'] = worker
        
        # Start Flask server in background thread if available
        if FLASK_AVAILABLE:
            print(f"üîß FLASK_AVAILABLE: {FLASK_AVAILABLE}")
            print(f"üîß Flask app object: {app}")
            print(f"üîß run_flask_server function: {run_flask_server}")
            
            flask_thread = threading.Thread(target=run_flask_server, daemon=True)
            flask_thread.start()
            print("‚úÖ Flask server thread started on port 7860")
            print("‚úÖ Thread-safe timeout mechanisms enabled")
            
            # ‚úÖ NEW: Wait for Flask to be ready, then auto-register
            print("‚è≥ Waiting for Flask server to be ready...")
            time.sleep(8)  # Give Flask time to fully start
            
            print("üåê Attempting worker URL auto-registration...")
            registration_success = auto_register_worker_url()
            
            if registration_success:
                print("‚úÖ Worker URL auto-registration successful")
                # Start periodic monitoring to keep registration fresh
                start_periodic_health_monitoring()
                print("üéØ System ready - edge functions can now find this worker automatically")
            else:
                print("‚ö†Ô∏è Auto-registration failed")
                print("üí° Admin can manually update URL at admin panel if needed")
                print("üîÑ Will retry in periodic monitoring...")
                # Still start monitoring for retry attempts
                start_periodic_health_monitoring()
        else:
            print("‚ö†Ô∏è Flask server not started - Flask not available")
        
        # Start main worker loop
        print("üé¨ Starting WAN worker main loop...")
        worker.run_with_enhanced_diagnostics()
        
    except KeyboardInterrupt:
        print("üõë Worker stopped by user")
        
        # ‚úÖ NEW: Deactivate URL registration on shutdown
        try:
            detected_url = detect_runpod_url()
            if detected_url:
                requests.patch(
                    f"{os.environ['SUPABASE_URL']}/rest/v1/system_config",
                    json={
                        "key": "worker_settings", 
                        "value": {"worker_status": "inactive", "last_updated": datetime.now().isoformat()}
                    },
                    headers={
                        "Authorization": f"Bearer {os.environ['SUPABASE_SERVICE_KEY']}",
                        "Content-Type": "application/json"
                    },
                    timeout=5
                )
                print("‚úÖ Worker URL deactivated on shutdown")
        except:
            pass
            
    except Exception as e:
        print(f"‚ùå Worker startup failed: {e}")
        import traceback
        traceback.print_exc()
        exit(1)
    finally:
        print("üëã Enhanced WAN 1.3B Worker (Base model only) shutdown complete")